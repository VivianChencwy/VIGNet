2025-11-12 09:09:49,251 - INFO - Log file: ./logs/trial8_cv0_RGS_20251112_090949.log
2025-11-12 09:09:49,251 - INFO - START TRAINING TRIAL 8 CV 0 - Task: RGS
2025-11-12 09:09:49,251 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 09:09:49,251 - INFO - Loading dataset...
2025-11-12 09:09:49,517 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 09:09:49,517 - INFO - Initializing VIGNet model...
2025-11-12 09:09:49,520 - INFO - Number of batch iterations per epoch: 113
2025-11-12 09:09:57,109 - INFO - Epoch: 1, Training Loss: 0.8180
2025-11-12 09:10:05,162 - INFO - Epoch: 2, Training Loss: 0.6097
2025-11-12 09:10:13,879 - INFO - Epoch: 3, Training Loss: 0.6036
2025-11-12 09:10:22,994 - INFO - Epoch: 4, Training Loss: 0.5957
2025-11-12 09:10:30,858 - INFO - Epoch: 5, Training Loss: 0.5949
2025-11-12 09:10:38,747 - INFO - Epoch: 6, Training Loss: 0.5938
2025-11-12 09:10:46,910 - INFO - Epoch: 7, Training Loss: 0.5935
2025-11-12 09:10:54,609 - INFO - Epoch: 8, Training Loss: 0.5910
2025-11-12 09:11:02,837 - INFO - Epoch: 9, Training Loss: 0.5908
2025-11-12 09:11:10,044 - INFO - Epoch: 10, Training Loss: 0.5937
2025-11-12 09:11:17,212 - INFO - Epoch: 11, Training Loss: 0.5930
2025-11-12 09:11:25,427 - INFO - Epoch: 12, Training Loss: 0.5905
2025-11-12 09:11:33,279 - INFO - Epoch: 13, Training Loss: 0.5911
2025-11-12 09:11:41,610 - INFO - Epoch: 14, Training Loss: 0.5904
2025-11-12 09:11:49,434 - INFO - Epoch: 15, Training Loss: 0.5907
2025-11-12 09:11:57,253 - INFO - Epoch: 16, Training Loss: 0.5907
2025-11-12 09:12:04,629 - INFO - Epoch: 17, Training Loss: 0.5919
2025-11-12 09:12:13,267 - INFO - Epoch: 18, Training Loss: 0.5899
2025-11-12 09:12:22,030 - INFO - Epoch: 19, Training Loss: 0.5911
2025-11-12 09:12:29,439 - INFO - Epoch: 20, Training Loss: 0.5912
2025-11-12 09:12:37,343 - INFO - Epoch: 21, Training Loss: 0.5912
2025-11-12 09:12:45,338 - INFO - Epoch: 22, Training Loss: 0.5904
2025-11-12 09:12:53,863 - INFO - Epoch: 23, Training Loss: 0.5906
2025-11-12 09:13:02,409 - INFO - Epoch: 24, Training Loss: 0.5897
2025-11-12 09:13:10,980 - INFO - Epoch: 25, Training Loss: 0.5896
2025-11-12 09:13:19,226 - INFO - Epoch: 26, Training Loss: 0.5893
2025-11-12 09:13:26,610 - INFO - Epoch: 27, Training Loss: 0.5882
2025-11-12 09:13:34,322 - INFO - Epoch: 28, Training Loss: 0.5897
2025-11-12 09:13:41,902 - INFO - Epoch: 29, Training Loss: 0.5849
2025-11-12 09:13:49,434 - INFO - Epoch: 30, Training Loss: 0.5841
2025-11-12 09:13:57,870 - INFO - Epoch: 31, Training Loss: 0.5777
2025-11-12 09:14:05,474 - INFO - Epoch: 32, Training Loss: 0.5684
2025-11-12 09:14:13,039 - INFO - Epoch: 33, Training Loss: 0.5425
2025-11-12 09:14:20,465 - INFO - Epoch: 34, Training Loss: 1.0331
2025-11-12 09:14:28,711 - INFO - Epoch: 35, Training Loss: 0.6015
2025-11-12 09:14:36,822 - INFO - Epoch: 36, Training Loss: 0.5899
2025-11-12 09:14:45,566 - INFO - Epoch: 37, Training Loss: 0.5977
2025-11-12 09:14:54,691 - INFO - Epoch: 38, Training Loss: 0.5929
2025-11-12 09:15:02,974 - INFO - Epoch: 39, Training Loss: 0.5952
2025-11-12 09:15:11,293 - INFO - Epoch: 40, Training Loss: 0.5898
2025-11-12 09:15:19,705 - INFO - Epoch: 41, Training Loss: 0.5920
2025-11-12 09:15:27,674 - INFO - Epoch: 42, Training Loss: 0.5894
2025-11-12 09:15:35,467 - INFO - Epoch: 43, Training Loss: 0.5873
2025-11-12 09:15:43,619 - INFO - Epoch: 44, Training Loss: 0.5878
2025-11-12 09:15:51,297 - INFO - Epoch: 45, Training Loss: 0.5859
2025-11-12 09:15:58,568 - INFO - Epoch: 46, Training Loss: 0.5912
2025-11-12 09:16:06,605 - INFO - Epoch: 47, Training Loss: 0.5884
2025-11-12 09:16:14,055 - INFO - Epoch: 48, Training Loss: 0.5898
2025-11-12 09:16:21,659 - INFO - Epoch: 49, Training Loss: 0.5903
2025-11-12 09:16:29,418 - INFO - Epoch: 50, Training Loss: 0.5889
2025-11-12 09:16:37,304 - INFO - Epoch: 51, Training Loss: 0.5880
2025-11-12 09:16:45,519 - INFO - Epoch: 52, Training Loss: 0.5878
2025-11-12 09:16:53,320 - INFO - Epoch: 53, Training Loss: 0.5875
2025-11-12 09:17:01,206 - INFO - Epoch: 54, Training Loss: 0.5843
2025-11-12 09:17:08,831 - INFO - Epoch: 55, Training Loss: 0.5878
2025-11-12 09:17:16,750 - INFO - Epoch: 56, Training Loss: 0.5865
2025-11-12 09:17:24,832 - INFO - Epoch: 57, Training Loss: 0.5859
2025-11-12 09:17:32,228 - INFO - Epoch: 58, Training Loss: 0.5863
2025-11-12 09:17:40,217 - INFO - Epoch: 59, Training Loss: 0.5872
2025-11-12 09:17:48,150 - INFO - Epoch: 60, Training Loss: 0.5839
2025-11-12 09:17:56,570 - INFO - Epoch: 61, Training Loss: 0.5881
2025-11-12 09:18:04,891 - INFO - Epoch: 62, Training Loss: 0.5858
2025-11-12 09:18:12,088 - INFO - Epoch: 63, Training Loss: 0.5818
2025-11-12 09:18:19,296 - INFO - Epoch: 64, Training Loss: 0.5860
2025-11-12 09:18:27,856 - INFO - Epoch: 65, Training Loss: 0.5881
2025-11-12 09:18:35,828 - INFO - Epoch: 66, Training Loss: 0.5839
2025-11-12 09:18:44,081 - INFO - Epoch: 67, Training Loss: 0.5832
2025-11-12 09:18:53,301 - INFO - Epoch: 68, Training Loss: 0.5750
2025-11-12 09:19:01,855 - INFO - Epoch: 69, Training Loss: 0.5674
2025-11-12 09:19:09,158 - INFO - Epoch: 70, Training Loss: 0.6028
2025-11-12 09:19:16,785 - INFO - Epoch: 71, Training Loss: 0.5710
2025-11-12 09:19:24,920 - INFO - Epoch: 72, Training Loss: 0.5683
2025-11-12 09:19:32,722 - INFO - Epoch: 73, Training Loss: 0.5647
2025-11-12 09:19:40,348 - INFO - Epoch: 74, Training Loss: 0.5596
2025-11-12 09:19:48,400 - INFO - Epoch: 75, Training Loss: 0.5619
2025-11-12 09:19:56,916 - INFO - Epoch: 76, Training Loss: 0.5535
2025-11-12 09:20:04,193 - INFO - Epoch: 77, Training Loss: 0.5324
2025-11-12 09:20:11,941 - INFO - Epoch: 78, Training Loss: 0.5360
2025-11-12 09:20:19,630 - INFO - Epoch: 79, Training Loss: 0.5178
2025-11-12 09:20:26,973 - INFO - Epoch: 80, Training Loss: 0.5015
2025-11-12 09:20:34,797 - INFO - Epoch: 81, Training Loss: 0.4924
2025-11-12 09:20:43,061 - INFO - Epoch: 82, Training Loss: 0.5082
2025-11-12 09:20:51,583 - INFO - Epoch: 83, Training Loss: 0.5069
2025-11-12 09:20:59,907 - INFO - Epoch: 84, Training Loss: 0.5145
2025-11-12 09:21:07,445 - INFO - Epoch: 85, Training Loss: 0.4999
2025-11-12 09:21:14,885 - INFO - Epoch: 86, Training Loss: 0.4875
2025-11-12 09:21:23,065 - INFO - Epoch: 87, Training Loss: 0.4923
2025-11-12 09:21:31,136 - INFO - Epoch: 88, Training Loss: 0.4800
2025-11-12 09:21:39,274 - INFO - Epoch: 89, Training Loss: 0.4867
2025-11-12 09:21:47,068 - INFO - Epoch: 90, Training Loss: 0.5220
2025-11-12 09:21:55,369 - INFO - Epoch: 91, Training Loss: 0.4953
2025-11-12 09:22:03,474 - INFO - Epoch: 92, Training Loss: 0.4889
2025-11-12 09:22:11,495 - INFO - Epoch: 93, Training Loss: 0.4862
2025-11-12 09:22:19,852 - INFO - Epoch: 94, Training Loss: 0.4852
2025-11-12 09:22:28,117 - INFO - Epoch: 95, Training Loss: 0.4837
2025-11-12 09:22:36,353 - INFO - Epoch: 96, Training Loss: 0.4786
2025-11-12 09:22:44,656 - INFO - Epoch: 97, Training Loss: 0.4780
2025-11-12 09:22:52,715 - INFO - Epoch: 98, Training Loss: 0.4765
2025-11-12 09:23:00,260 - INFO - Epoch: 99, Training Loss: 0.4790
2025-11-12 09:23:08,472 - INFO - Epoch: 100, Training Loss: 0.4739
2025-11-12 09:23:08,472 - INFO - Training completed for Trial 8 CV 0

