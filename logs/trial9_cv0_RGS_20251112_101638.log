2025-11-12 10:16:38,264 - INFO - Log file: ./logs/trial9_cv0_RGS_20251112_101638.log
2025-11-12 10:16:38,264 - INFO - START TRAINING TRIAL 9 CV 0 - Task: RGS
2025-11-12 10:16:38,264 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 10:16:38,264 - INFO - Loading dataset...
2025-11-12 10:16:38,413 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 10:16:38,413 - INFO - Initializing VIGNet model...
2025-11-12 10:16:38,416 - INFO - Number of batch iterations per epoch: 113
2025-11-12 10:16:46,814 - INFO - Epoch: 1, Training Loss: 1.1152
2025-11-12 10:16:53,769 - INFO - Epoch: 2, Training Loss: 0.8381
2025-11-12 10:17:02,247 - INFO - Epoch: 3, Training Loss: 0.7046
2025-11-12 10:17:10,422 - INFO - Epoch: 4, Training Loss: 0.7015
2025-11-12 10:17:18,229 - INFO - Epoch: 5, Training Loss: 0.6936
2025-11-12 10:17:26,150 - INFO - Epoch: 6, Training Loss: 0.6892
2025-11-12 10:17:34,138 - INFO - Epoch: 7, Training Loss: 0.6915
2025-11-12 10:17:42,155 - INFO - Epoch: 8, Training Loss: 0.6899
2025-11-12 10:17:50,299 - INFO - Epoch: 9, Training Loss: 0.6903
2025-11-12 10:17:58,312 - INFO - Epoch: 10, Training Loss: 0.6897
2025-11-12 10:18:06,786 - INFO - Epoch: 11, Training Loss: 0.6873
2025-11-12 10:18:14,426 - INFO - Epoch: 12, Training Loss: 0.6878
2025-11-12 10:18:22,678 - INFO - Epoch: 13, Training Loss: 0.6875
2025-11-12 10:18:30,657 - INFO - Epoch: 14, Training Loss: 0.6870
2025-11-12 10:18:39,061 - INFO - Epoch: 15, Training Loss: 0.6876
2025-11-12 10:18:47,038 - INFO - Epoch: 16, Training Loss: 0.6876
2025-11-12 10:18:55,132 - INFO - Epoch: 17, Training Loss: 0.6874
2025-11-12 10:19:03,183 - INFO - Epoch: 18, Training Loss: 0.6871
2025-11-12 10:19:11,939 - INFO - Epoch: 19, Training Loss: 0.6877
2025-11-12 10:19:19,138 - INFO - Epoch: 20, Training Loss: 0.6874
2025-11-12 10:19:28,196 - INFO - Epoch: 21, Training Loss: 0.6863
2025-11-12 10:19:36,790 - INFO - Epoch: 22, Training Loss: 0.6882
2025-11-12 10:19:44,981 - INFO - Epoch: 23, Training Loss: 0.6870
2025-11-12 10:19:52,537 - INFO - Epoch: 24, Training Loss: 0.6869
2025-11-12 10:20:00,839 - INFO - Epoch: 25, Training Loss: 0.6865
2025-11-12 10:20:08,418 - INFO - Epoch: 26, Training Loss: 0.6869
2025-11-12 10:20:16,323 - INFO - Epoch: 27, Training Loss: 0.6874
2025-11-12 10:20:25,177 - INFO - Epoch: 28, Training Loss: 0.6880
2025-11-12 10:20:33,600 - INFO - Epoch: 29, Training Loss: 0.6870
2025-11-12 10:20:41,318 - INFO - Epoch: 30, Training Loss: 0.6876
2025-11-12 10:20:49,116 - INFO - Epoch: 31, Training Loss: 0.6868
2025-11-12 10:20:56,396 - INFO - Epoch: 32, Training Loss: 0.6869
2025-11-12 10:21:04,608 - INFO - Epoch: 33, Training Loss: 0.6865
2025-11-12 10:21:13,347 - INFO - Epoch: 34, Training Loss: 0.6865
2025-11-12 10:21:21,494 - INFO - Epoch: 35, Training Loss: 0.6882
2025-11-12 10:21:29,932 - INFO - Epoch: 36, Training Loss: 0.6867
2025-11-12 10:21:37,278 - INFO - Epoch: 37, Training Loss: 0.6868
2025-11-12 10:21:44,356 - INFO - Epoch: 38, Training Loss: 0.6872
2025-11-12 10:21:51,220 - INFO - Epoch: 39, Training Loss: 0.6868
2025-11-12 10:21:58,812 - INFO - Epoch: 40, Training Loss: 0.6873
2025-11-12 10:22:06,410 - INFO - Epoch: 41, Training Loss: 0.6872
2025-11-12 10:22:14,584 - INFO - Epoch: 42, Training Loss: 0.6866
2025-11-12 10:22:22,783 - INFO - Epoch: 43, Training Loss: 0.6875
2025-11-12 10:22:31,128 - INFO - Epoch: 44, Training Loss: 0.6871
2025-11-12 10:22:39,056 - INFO - Epoch: 45, Training Loss: 0.6865
2025-11-12 10:22:47,776 - INFO - Epoch: 46, Training Loss: 0.6875
2025-11-12 10:22:55,928 - INFO - Epoch: 47, Training Loss: 0.6870
2025-11-12 10:23:03,342 - INFO - Epoch: 48, Training Loss: 0.6865
2025-11-12 10:23:11,225 - INFO - Epoch: 49, Training Loss: 0.6869
2025-11-12 10:23:18,922 - INFO - Epoch: 50, Training Loss: 0.6864
2025-11-12 10:23:26,751 - INFO - Epoch: 51, Training Loss: 0.6866
2025-11-12 10:23:34,775 - INFO - Epoch: 52, Training Loss: 0.6875
2025-11-12 10:23:42,128 - INFO - Epoch: 53, Training Loss: 0.6866
2025-11-12 10:23:50,693 - INFO - Epoch: 54, Training Loss: 0.6863
2025-11-12 10:23:58,759 - INFO - Epoch: 55, Training Loss: 0.6872
2025-11-12 10:24:06,589 - INFO - Epoch: 56, Training Loss: 0.6882
2025-11-12 10:24:14,353 - INFO - Epoch: 57, Training Loss: 0.6860
2025-11-12 10:24:22,174 - INFO - Epoch: 58, Training Loss: 0.6861
2025-11-12 10:24:30,266 - INFO - Epoch: 59, Training Loss: 0.6863
2025-11-12 10:24:38,705 - INFO - Epoch: 60, Training Loss: 0.6852
2025-11-12 10:24:47,794 - INFO - Epoch: 61, Training Loss: 0.6849
2025-11-12 10:24:56,327 - INFO - Epoch: 62, Training Loss: 0.6844
2025-11-12 10:25:03,896 - INFO - Epoch: 63, Training Loss: 0.6843
2025-11-12 10:25:12,152 - INFO - Epoch: 64, Training Loss: 0.6834
2025-11-12 10:25:20,801 - INFO - Epoch: 65, Training Loss: 0.6819
2025-11-12 10:25:29,055 - INFO - Epoch: 66, Training Loss: 0.6800
2025-11-12 10:25:37,290 - INFO - Epoch: 67, Training Loss: 0.6779
2025-11-12 10:25:45,085 - INFO - Epoch: 68, Training Loss: 0.6796
2025-11-12 10:25:53,213 - INFO - Epoch: 69, Training Loss: 0.6763
2025-11-12 10:26:01,596 - INFO - Epoch: 70, Training Loss: 0.6768
2025-11-12 10:26:09,299 - INFO - Epoch: 71, Training Loss: 0.6747
2025-11-12 10:26:18,303 - INFO - Epoch: 72, Training Loss: 0.6752
2025-11-12 10:26:26,179 - INFO - Epoch: 73, Training Loss: 0.6732
2025-11-12 10:26:34,308 - INFO - Epoch: 74, Training Loss: 0.6737
2025-11-12 10:26:43,037 - INFO - Epoch: 75, Training Loss: 0.6732
2025-11-12 10:26:51,057 - INFO - Epoch: 76, Training Loss: 0.6731
2025-11-12 10:26:58,775 - INFO - Epoch: 77, Training Loss: 0.6740
2025-11-12 10:27:07,503 - INFO - Epoch: 78, Training Loss: 0.6731
2025-11-12 10:27:15,971 - INFO - Epoch: 79, Training Loss: 0.6747
2025-11-12 10:27:23,294 - INFO - Epoch: 80, Training Loss: 0.6729
2025-11-12 10:27:31,671 - INFO - Epoch: 81, Training Loss: 0.6740
2025-11-12 10:27:39,524 - INFO - Epoch: 82, Training Loss: 0.6731
2025-11-12 10:27:47,635 - INFO - Epoch: 83, Training Loss: 0.6729
2025-11-12 10:27:55,580 - INFO - Epoch: 84, Training Loss: 0.6728
2025-11-12 10:28:03,857 - INFO - Epoch: 85, Training Loss: 0.6727
2025-11-12 10:28:11,142 - INFO - Epoch: 86, Training Loss: 0.6722
2025-11-12 10:28:20,248 - INFO - Epoch: 87, Training Loss: 0.6724
2025-11-12 10:28:28,439 - INFO - Epoch: 88, Training Loss: 0.6729
2025-11-12 10:28:35,685 - INFO - Epoch: 89, Training Loss: 0.6731
2025-11-12 10:28:43,201 - INFO - Epoch: 90, Training Loss: 0.6831
2025-11-12 10:28:51,202 - INFO - Epoch: 91, Training Loss: 0.6731
2025-11-12 10:28:58,862 - INFO - Epoch: 92, Training Loss: 0.6729
2025-11-12 10:29:06,495 - INFO - Epoch: 93, Training Loss: 0.6721
2025-11-12 10:29:14,601 - INFO - Epoch: 94, Training Loss: 0.6725
2025-11-12 10:29:23,046 - INFO - Epoch: 95, Training Loss: 0.6726
2025-11-12 10:29:31,063 - INFO - Epoch: 96, Training Loss: 0.6712
2025-11-12 10:29:39,165 - INFO - Epoch: 97, Training Loss: 0.6723
2025-11-12 10:29:46,628 - INFO - Epoch: 98, Training Loss: 0.6719
2025-11-12 10:29:55,013 - INFO - Epoch: 99, Training Loss: 0.6711
2025-11-12 10:30:02,498 - INFO - Epoch: 100, Training Loss: 0.6715
2025-11-12 10:30:02,499 - INFO - Training completed for Trial 9 CV 0

