2025-11-12 02:43:58,068 - INFO - Log file: ./logs/trial2_cv1_RGS_20251112_024358.log
2025-11-12 02:43:58,068 - INFO - START TRAINING TRIAL 2 CV 1 - Task: RGS
2025-11-12 02:43:58,068 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 02:43:58,068 - INFO - Loading dataset...
2025-11-12 02:43:58,157 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 02:43:58,157 - INFO - Initializing VIGNet model...
2025-11-12 02:43:58,160 - INFO - Number of batch iterations per epoch: 113
2025-11-12 02:44:05,769 - INFO - Epoch: 1, Training Loss: 0.7769
2025-11-12 02:44:14,001 - INFO - Epoch: 2, Training Loss: 0.6017
2025-11-12 02:44:22,044 - INFO - Epoch: 3, Training Loss: 0.5982
2025-11-12 02:44:30,019 - INFO - Epoch: 4, Training Loss: 0.5974
2025-11-12 02:44:37,398 - INFO - Epoch: 5, Training Loss: 0.5977
2025-11-12 02:44:45,114 - INFO - Epoch: 6, Training Loss: 0.5960
2025-11-12 02:44:52,288 - INFO - Epoch: 7, Training Loss: 0.5974
2025-11-12 02:45:00,519 - INFO - Epoch: 8, Training Loss: 0.5957
2025-11-12 02:45:08,669 - INFO - Epoch: 9, Training Loss: 0.5968
2025-11-12 02:45:17,199 - INFO - Epoch: 10, Training Loss: 0.5952
2025-11-12 02:45:25,576 - INFO - Epoch: 11, Training Loss: 0.5952
2025-11-12 02:45:33,388 - INFO - Epoch: 12, Training Loss: 0.5960
2025-11-12 02:45:41,463 - INFO - Epoch: 13, Training Loss: 0.5959
2025-11-12 02:45:48,972 - INFO - Epoch: 14, Training Loss: 0.5945
2025-11-12 02:45:57,034 - INFO - Epoch: 15, Training Loss: 0.5974
2025-11-12 02:46:04,486 - INFO - Epoch: 16, Training Loss: 0.5961
2025-11-12 02:46:12,405 - INFO - Epoch: 17, Training Loss: 0.5965
2025-11-12 02:46:19,892 - INFO - Epoch: 18, Training Loss: 0.5956
2025-11-12 02:46:28,495 - INFO - Epoch: 19, Training Loss: 0.5949
2025-11-12 02:46:36,388 - INFO - Epoch: 20, Training Loss: 0.5959
2025-11-12 02:46:44,408 - INFO - Epoch: 21, Training Loss: 0.5978
2025-11-12 02:46:51,653 - INFO - Epoch: 22, Training Loss: 0.5951
2025-11-12 02:46:59,154 - INFO - Epoch: 23, Training Loss: 0.5960
2025-11-12 02:47:07,720 - INFO - Epoch: 24, Training Loss: 0.5943
2025-11-12 02:47:15,833 - INFO - Epoch: 25, Training Loss: 0.5957
2025-11-12 02:47:24,122 - INFO - Epoch: 26, Training Loss: 0.5957
2025-11-12 02:47:32,954 - INFO - Epoch: 27, Training Loss: 0.5959
2025-11-12 02:47:40,721 - INFO - Epoch: 28, Training Loss: 0.5957
2025-11-12 02:47:49,000 - INFO - Epoch: 29, Training Loss: 0.5944
2025-11-12 02:47:58,019 - INFO - Epoch: 30, Training Loss: 0.5953
2025-11-12 02:48:05,308 - INFO - Epoch: 31, Training Loss: 0.5956
2025-11-12 02:48:13,566 - INFO - Epoch: 32, Training Loss: 0.5962
2025-11-12 02:48:21,887 - INFO - Epoch: 33, Training Loss: 0.5955
2025-11-12 02:48:30,187 - INFO - Epoch: 34, Training Loss: 0.5949
2025-11-12 02:48:38,985 - INFO - Epoch: 35, Training Loss: 0.5957
2025-11-12 02:48:46,785 - INFO - Epoch: 36, Training Loss: 0.5960
2025-11-12 02:48:55,047 - INFO - Epoch: 37, Training Loss: 0.5957
2025-11-12 02:49:03,364 - INFO - Epoch: 38, Training Loss: 0.5956
2025-11-12 02:49:11,349 - INFO - Epoch: 39, Training Loss: 0.5963
2025-11-12 02:49:19,221 - INFO - Epoch: 40, Training Loss: 0.5960
2025-11-12 02:49:27,159 - INFO - Epoch: 41, Training Loss: 0.5943
2025-11-12 02:49:35,888 - INFO - Epoch: 42, Training Loss: 0.5951
2025-11-12 02:49:43,768 - INFO - Epoch: 43, Training Loss: 0.5943
2025-11-12 02:49:52,755 - INFO - Epoch: 44, Training Loss: 0.5935
2025-11-12 02:50:00,775 - INFO - Epoch: 45, Training Loss: 0.5959
2025-11-12 02:50:08,186 - INFO - Epoch: 46, Training Loss: 0.5953
2025-11-12 02:50:15,863 - INFO - Epoch: 47, Training Loss: 0.5961
2025-11-12 02:50:24,192 - INFO - Epoch: 48, Training Loss: 0.5950
2025-11-12 02:50:32,694 - INFO - Epoch: 49, Training Loss: 0.5953
2025-11-12 02:50:40,869 - INFO - Epoch: 50, Training Loss: 0.5961
2025-11-12 02:50:48,563 - INFO - Epoch: 51, Training Loss: 0.5956
2025-11-12 02:50:56,398 - INFO - Epoch: 52, Training Loss: 0.5965
2025-11-12 02:51:04,323 - INFO - Epoch: 53, Training Loss: 0.5962
2025-11-12 02:51:13,022 - INFO - Epoch: 54, Training Loss: 0.5951
2025-11-12 02:51:20,953 - INFO - Epoch: 55, Training Loss: 0.5958
2025-11-12 02:51:29,712 - INFO - Epoch: 56, Training Loss: 0.5946
2025-11-12 02:51:37,329 - INFO - Epoch: 57, Training Loss: 0.5962
2025-11-12 02:51:45,795 - INFO - Epoch: 58, Training Loss: 0.5940
2025-11-12 02:51:53,468 - INFO - Epoch: 59, Training Loss: 0.5954
2025-11-12 02:52:01,931 - INFO - Epoch: 60, Training Loss: 0.5955
2025-11-12 02:52:09,561 - INFO - Epoch: 61, Training Loss: 0.5957
2025-11-12 02:52:17,732 - INFO - Epoch: 62, Training Loss: 0.5960
2025-11-12 02:52:25,649 - INFO - Epoch: 63, Training Loss: 0.5960
2025-11-12 02:52:33,599 - INFO - Epoch: 64, Training Loss: 0.5952
2025-11-12 02:52:42,272 - INFO - Epoch: 65, Training Loss: 0.5953
2025-11-12 02:52:49,931 - INFO - Epoch: 66, Training Loss: 0.5958
2025-11-12 02:52:58,285 - INFO - Epoch: 67, Training Loss: 0.5964
2025-11-12 02:53:06,265 - INFO - Epoch: 68, Training Loss: 0.5957
2025-11-12 02:53:14,828 - INFO - Epoch: 69, Training Loss: 0.5952
2025-11-12 02:53:22,333 - INFO - Epoch: 70, Training Loss: 0.5954
2025-11-12 02:53:30,462 - INFO - Epoch: 71, Training Loss: 0.5951
2025-11-12 02:53:37,749 - INFO - Epoch: 72, Training Loss: 0.5951
2025-11-12 02:53:46,155 - INFO - Epoch: 73, Training Loss: 0.5965
2025-11-12 02:53:53,879 - INFO - Epoch: 74, Training Loss: 0.5952
2025-11-12 02:54:02,583 - INFO - Epoch: 75, Training Loss: 0.5955
2025-11-12 02:54:10,293 - INFO - Epoch: 76, Training Loss: 0.5950
2025-11-12 02:54:18,463 - INFO - Epoch: 77, Training Loss: 0.5962
2025-11-12 02:54:26,637 - INFO - Epoch: 78, Training Loss: 0.5953
2025-11-12 02:54:34,640 - INFO - Epoch: 79, Training Loss: 0.5948
2025-11-12 02:54:43,708 - INFO - Epoch: 80, Training Loss: 0.5957
2025-11-12 02:54:52,425 - INFO - Epoch: 81, Training Loss: 0.5951
2025-11-12 02:55:00,241 - INFO - Epoch: 82, Training Loss: 0.5959
2025-11-12 02:55:07,908 - INFO - Epoch: 83, Training Loss: 0.5948
2025-11-12 02:55:15,786 - INFO - Epoch: 84, Training Loss: 0.5953
2025-11-12 02:55:24,531 - INFO - Epoch: 85, Training Loss: 0.5943
2025-11-12 02:55:32,502 - INFO - Epoch: 86, Training Loss: 0.5888
2025-11-12 02:55:41,533 - INFO - Epoch: 87, Training Loss: 0.5855
2025-11-12 02:55:50,178 - INFO - Epoch: 88, Training Loss: 0.5850
2025-11-12 02:55:57,471 - INFO - Epoch: 89, Training Loss: 0.5852
2025-11-12 02:56:05,030 - INFO - Epoch: 90, Training Loss: 0.5788
2025-11-12 02:56:12,501 - INFO - Epoch: 91, Training Loss: 0.5776
2025-11-12 02:56:19,864 - INFO - Epoch: 92, Training Loss: 0.5729
2025-11-12 02:56:27,436 - INFO - Epoch: 93, Training Loss: 0.5707
2025-11-12 02:56:35,584 - INFO - Epoch: 94, Training Loss: 0.5712
2025-11-12 02:56:43,620 - INFO - Epoch: 95, Training Loss: 0.5663
2025-11-12 02:56:52,211 - INFO - Epoch: 96, Training Loss: 0.5636
2025-11-12 02:57:00,339 - INFO - Epoch: 97, Training Loss: 0.5798
2025-11-12 02:57:08,339 - INFO - Epoch: 98, Training Loss: 0.5773
2025-11-12 02:57:16,112 - INFO - Epoch: 99, Training Loss: 0.5725
2025-11-12 02:57:23,849 - INFO - Epoch: 100, Training Loss: 0.5797
2025-11-12 02:57:23,850 - INFO - Training completed for Trial 2 CV 1

