2025-11-12 10:43:27,435 - INFO - Log file: ./logs/trial9_cv2_RGS_20251112_104327.log
2025-11-12 10:43:27,435 - INFO - START TRAINING TRIAL 9 CV 2 - Task: RGS
2025-11-12 10:43:27,435 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 10:43:27,435 - INFO - Loading dataset...
2025-11-12 10:43:27,534 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 10:43:27,534 - INFO - Initializing VIGNet model...
2025-11-12 10:43:27,537 - INFO - Number of batch iterations per epoch: 113
2025-11-12 10:43:36,149 - INFO - Epoch: 1, Training Loss: 1.0273
2025-11-12 10:43:44,461 - INFO - Epoch: 2, Training Loss: 0.6981
2025-11-12 10:43:52,658 - INFO - Epoch: 3, Training Loss: 0.6935
2025-11-12 10:44:01,210 - INFO - Epoch: 4, Training Loss: 0.6895
2025-11-12 10:44:09,172 - INFO - Epoch: 5, Training Loss: 0.6900
2025-11-12 10:44:17,458 - INFO - Epoch: 6, Training Loss: 0.6874
2025-11-12 10:44:25,330 - INFO - Epoch: 7, Training Loss: 0.6882
2025-11-12 10:44:33,670 - INFO - Epoch: 8, Training Loss: 0.6870
2025-11-12 10:44:42,499 - INFO - Epoch: 9, Training Loss: 0.6867
2025-11-12 10:44:50,508 - INFO - Epoch: 10, Training Loss: 0.6888
2025-11-12 10:44:58,596 - INFO - Epoch: 11, Training Loss: 0.6870
2025-11-12 10:45:05,922 - INFO - Epoch: 12, Training Loss: 0.6860
2025-11-12 10:45:13,625 - INFO - Epoch: 13, Training Loss: 0.6864
2025-11-12 10:45:21,618 - INFO - Epoch: 14, Training Loss: 0.6868
2025-11-12 10:45:29,376 - INFO - Epoch: 15, Training Loss: 0.6876
2025-11-12 10:45:37,683 - INFO - Epoch: 16, Training Loss: 0.6866
2025-11-12 10:45:45,677 - INFO - Epoch: 17, Training Loss: 0.6863
2025-11-12 10:45:54,053 - INFO - Epoch: 18, Training Loss: 0.6875
2025-11-12 10:46:02,548 - INFO - Epoch: 19, Training Loss: 0.6867
2025-11-12 10:46:11,090 - INFO - Epoch: 20, Training Loss: 0.6864
2025-11-12 10:46:19,189 - INFO - Epoch: 21, Training Loss: 0.6869
2025-11-12 10:46:27,154 - INFO - Epoch: 22, Training Loss: 0.6858
2025-11-12 10:46:35,867 - INFO - Epoch: 23, Training Loss: 0.6872
2025-11-12 10:46:44,025 - INFO - Epoch: 24, Training Loss: 0.6870
2025-11-12 10:46:51,979 - INFO - Epoch: 25, Training Loss: 0.6873
2025-11-12 10:47:00,305 - INFO - Epoch: 26, Training Loss: 0.6868
2025-11-12 10:47:08,395 - INFO - Epoch: 27, Training Loss: 0.6873
2025-11-12 10:47:15,941 - INFO - Epoch: 28, Training Loss: 0.6867
2025-11-12 10:47:23,613 - INFO - Epoch: 29, Training Loss: 0.6862
2025-11-12 10:47:31,164 - INFO - Epoch: 30, Training Loss: 0.6864
2025-11-12 10:47:39,325 - INFO - Epoch: 31, Training Loss: 0.6871
2025-11-12 10:47:46,988 - INFO - Epoch: 32, Training Loss: 0.6866
2025-11-12 10:47:55,042 - INFO - Epoch: 33, Training Loss: 0.6862
2025-11-12 10:48:02,732 - INFO - Epoch: 34, Training Loss: 0.6874
2025-11-12 10:48:10,220 - INFO - Epoch: 35, Training Loss: 0.6866
2025-11-12 10:48:18,424 - INFO - Epoch: 36, Training Loss: 0.6862
2025-11-12 10:48:25,999 - INFO - Epoch: 37, Training Loss: 0.6883
2025-11-12 10:48:33,611 - INFO - Epoch: 38, Training Loss: 0.6868
2025-11-12 10:48:41,640 - INFO - Epoch: 39, Training Loss: 0.6860
2025-11-12 10:48:49,616 - INFO - Epoch: 40, Training Loss: 0.6864
2025-11-12 10:48:57,369 - INFO - Epoch: 41, Training Loss: 0.6865
2025-11-12 10:49:05,638 - INFO - Epoch: 42, Training Loss: 0.6861
2025-11-12 10:49:13,191 - INFO - Epoch: 43, Training Loss: 0.6859
2025-11-12 10:49:21,724 - INFO - Epoch: 44, Training Loss: 0.6857
2025-11-12 10:49:30,438 - INFO - Epoch: 45, Training Loss: 0.6864
2025-11-12 10:49:38,300 - INFO - Epoch: 46, Training Loss: 0.6869
2025-11-12 10:49:46,595 - INFO - Epoch: 47, Training Loss: 0.6856
2025-11-12 10:49:54,704 - INFO - Epoch: 48, Training Loss: 0.6853
2025-11-12 10:50:01,713 - INFO - Epoch: 49, Training Loss: 0.6839
2025-11-12 10:50:09,758 - INFO - Epoch: 50, Training Loss: 0.6801
2025-11-12 10:50:18,097 - INFO - Epoch: 51, Training Loss: 0.6752
2025-11-12 10:50:26,107 - INFO - Epoch: 52, Training Loss: 0.6735
2025-11-12 10:50:34,109 - INFO - Epoch: 53, Training Loss: 0.6731
2025-11-12 10:50:41,992 - INFO - Epoch: 54, Training Loss: 0.6730
2025-11-12 10:50:49,867 - INFO - Epoch: 55, Training Loss: 0.6725
2025-11-12 10:50:58,273 - INFO - Epoch: 56, Training Loss: 0.6718
2025-11-12 10:51:05,671 - INFO - Epoch: 57, Training Loss: 0.6710
2025-11-12 10:51:13,577 - INFO - Epoch: 58, Training Loss: 0.6716
2025-11-12 10:51:22,299 - INFO - Epoch: 59, Training Loss: 0.6718
2025-11-12 10:51:30,691 - INFO - Epoch: 60, Training Loss: 0.6717
2025-11-12 10:51:38,521 - INFO - Epoch: 61, Training Loss: 0.6710
2025-11-12 10:51:46,342 - INFO - Epoch: 62, Training Loss: 0.6722
2025-11-12 10:51:54,107 - INFO - Epoch: 63, Training Loss: 0.6713
2025-11-12 10:52:02,292 - INFO - Epoch: 64, Training Loss: 0.6715
2025-11-12 10:52:09,897 - INFO - Epoch: 65, Training Loss: 0.6713
2025-11-12 10:52:18,918 - INFO - Epoch: 66, Training Loss: 0.6708
2025-11-12 10:52:26,112 - INFO - Epoch: 67, Training Loss: 0.6715
2025-11-12 10:52:34,123 - INFO - Epoch: 68, Training Loss: 0.6711
2025-11-12 10:52:42,131 - INFO - Epoch: 69, Training Loss: 0.6711
2025-11-12 10:52:50,085 - INFO - Epoch: 70, Training Loss: 0.6704
2025-11-12 10:52:57,805 - INFO - Epoch: 71, Training Loss: 0.6702
2025-11-12 10:53:06,163 - INFO - Epoch: 72, Training Loss: 0.6703
2025-11-12 10:53:14,462 - INFO - Epoch: 73, Training Loss: 0.6704
2025-11-12 10:53:23,191 - INFO - Epoch: 74, Training Loss: 0.6701
2025-11-12 10:53:31,743 - INFO - Epoch: 75, Training Loss: 0.6701
2025-11-12 10:53:39,875 - INFO - Epoch: 76, Training Loss: 0.6699
2025-11-12 10:53:47,859 - INFO - Epoch: 77, Training Loss: 0.6703
2025-11-12 10:53:55,365 - INFO - Epoch: 78, Training Loss: 0.6706
2025-11-12 10:54:03,109 - INFO - Epoch: 79, Training Loss: 0.6699
2025-11-12 10:54:10,804 - INFO - Epoch: 80, Training Loss: 0.6701
2025-11-12 10:54:19,362 - INFO - Epoch: 81, Training Loss: 0.6694
2025-11-12 10:54:27,304 - INFO - Epoch: 82, Training Loss: 0.6696
2025-11-12 10:54:34,926 - INFO - Epoch: 83, Training Loss: 0.6689
2025-11-12 10:54:42,581 - INFO - Epoch: 84, Training Loss: 0.6693
2025-11-12 10:54:50,305 - INFO - Epoch: 85, Training Loss: 0.6694
2025-11-12 10:54:58,763 - INFO - Epoch: 86, Training Loss: 0.6686
2025-11-12 10:55:06,343 - INFO - Epoch: 87, Training Loss: 0.6691
2025-11-12 10:55:13,806 - INFO - Epoch: 88, Training Loss: 0.6689
2025-11-12 10:55:22,770 - INFO - Epoch: 89, Training Loss: 0.6684
2025-11-12 10:55:30,505 - INFO - Epoch: 90, Training Loss: 0.6689
2025-11-12 10:55:38,324 - INFO - Epoch: 91, Training Loss: 0.6690
2025-11-12 10:55:46,370 - INFO - Epoch: 92, Training Loss: 0.6690
2025-11-12 10:55:54,469 - INFO - Epoch: 93, Training Loss: 0.6695
2025-11-12 10:56:02,520 - INFO - Epoch: 94, Training Loss: 0.6684
2025-11-12 10:56:10,693 - INFO - Epoch: 95, Training Loss: 0.6683
2025-11-12 10:56:18,609 - INFO - Epoch: 96, Training Loss: 0.6679
2025-11-12 10:56:26,766 - INFO - Epoch: 97, Training Loss: 0.6681
2025-11-12 10:56:35,233 - INFO - Epoch: 98, Training Loss: 0.6683
2025-11-12 10:56:43,692 - INFO - Epoch: 99, Training Loss: 0.6676
2025-11-12 10:56:52,476 - INFO - Epoch: 100, Training Loss: 0.6676
2025-11-12 10:56:52,476 - INFO - Training completed for Trial 9 CV 2

