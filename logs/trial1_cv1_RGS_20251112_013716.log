2025-11-12 01:37:16,959 - INFO - Log file: ./logs/trial1_cv1_RGS_20251112_013716.log
2025-11-12 01:37:16,959 - INFO - START TRAINING TRIAL 1 CV 1 - Task: RGS
2025-11-12 01:37:16,959 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 01:37:16,959 - INFO - Loading dataset...
2025-11-12 01:37:17,050 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 01:37:17,050 - INFO - Initializing VIGNet model...
2025-11-12 01:37:17,053 - INFO - Number of batch iterations per epoch: 113
2025-11-12 01:37:25,073 - INFO - Epoch: 1, Training Loss: 1.4450
2025-11-12 01:37:33,050 - INFO - Epoch: 2, Training Loss: 0.6550
2025-11-12 01:37:42,847 - INFO - Epoch: 3, Training Loss: 0.6550
2025-11-12 01:37:50,429 - INFO - Epoch: 4, Training Loss: 0.6482
2025-11-12 01:37:58,265 - INFO - Epoch: 5, Training Loss: 0.6489
2025-11-12 01:38:07,012 - INFO - Epoch: 6, Training Loss: 0.6449
2025-11-12 01:38:14,627 - INFO - Epoch: 7, Training Loss: 0.6476
2025-11-12 01:38:21,981 - INFO - Epoch: 8, Training Loss: 0.6487
2025-11-12 01:38:29,402 - INFO - Epoch: 9, Training Loss: 0.6418
2025-11-12 01:38:37,406 - INFO - Epoch: 10, Training Loss: 0.6438
2025-11-12 01:38:44,941 - INFO - Epoch: 11, Training Loss: 0.6428
2025-11-12 01:38:53,364 - INFO - Epoch: 12, Training Loss: 0.6449
2025-11-12 01:39:01,284 - INFO - Epoch: 13, Training Loss: 0.6406
2025-11-12 01:39:08,795 - INFO - Epoch: 14, Training Loss: 0.6346
2025-11-12 01:39:17,372 - INFO - Epoch: 15, Training Loss: 0.6340
2025-11-12 01:39:25,406 - INFO - Epoch: 16, Training Loss: 0.6277
2025-11-12 01:39:33,427 - INFO - Epoch: 17, Training Loss: 0.6292
2025-11-12 01:39:41,221 - INFO - Epoch: 18, Training Loss: 0.6243
2025-11-12 01:39:48,528 - INFO - Epoch: 19, Training Loss: 0.6174
2025-11-12 01:39:56,402 - INFO - Epoch: 20, Training Loss: 0.6311
2025-11-12 01:40:04,834 - INFO - Epoch: 21, Training Loss: 0.6208
2025-11-12 01:40:12,610 - INFO - Epoch: 22, Training Loss: 0.6160
2025-11-12 01:40:20,649 - INFO - Epoch: 23, Training Loss: 0.6026
2025-11-12 01:40:28,997 - INFO - Epoch: 24, Training Loss: 0.6037
2025-11-12 01:40:37,422 - INFO - Epoch: 25, Training Loss: 0.5548
2025-11-12 01:40:45,144 - INFO - Epoch: 26, Training Loss: 0.5395
2025-11-12 01:40:53,114 - INFO - Epoch: 27, Training Loss: 0.5211
2025-11-12 01:41:00,913 - INFO - Epoch: 28, Training Loss: 0.5436
2025-11-12 01:41:09,726 - INFO - Epoch: 29, Training Loss: 0.4889
2025-11-12 01:41:17,258 - INFO - Epoch: 30, Training Loss: 0.5109
2025-11-12 01:41:25,241 - INFO - Epoch: 31, Training Loss: 0.4945
2025-11-12 01:41:32,809 - INFO - Epoch: 32, Training Loss: 0.4878
2025-11-12 01:41:41,108 - INFO - Epoch: 33, Training Loss: 0.4724
2025-11-12 01:41:49,772 - INFO - Epoch: 34, Training Loss: 0.4829
2025-11-12 01:41:57,266 - INFO - Epoch: 35, Training Loss: 0.4855
2025-11-12 01:42:05,357 - INFO - Epoch: 36, Training Loss: 0.4946
2025-11-12 01:42:13,716 - INFO - Epoch: 37, Training Loss: 0.8609
2025-11-12 01:42:21,361 - INFO - Epoch: 38, Training Loss: 0.5241
2025-11-12 01:42:29,287 - INFO - Epoch: 39, Training Loss: 0.5038
2025-11-12 01:42:36,435 - INFO - Epoch: 40, Training Loss: 0.4878
2025-11-12 01:42:44,716 - INFO - Epoch: 41, Training Loss: 0.4852
2025-11-12 01:42:53,111 - INFO - Epoch: 42, Training Loss: 0.4982
2025-11-12 01:43:01,316 - INFO - Epoch: 43, Training Loss: 0.5075
2025-11-12 01:43:09,497 - INFO - Epoch: 44, Training Loss: 0.4920
2025-11-12 01:43:17,109 - INFO - Epoch: 45, Training Loss: 0.4833
2025-11-12 01:43:25,052 - INFO - Epoch: 46, Training Loss: 0.4902
2025-11-12 01:43:32,953 - INFO - Epoch: 47, Training Loss: 0.4868
2025-11-12 01:43:40,405 - INFO - Epoch: 48, Training Loss: 0.4721
2025-11-12 01:43:48,308 - INFO - Epoch: 49, Training Loss: 0.4917
2025-11-12 01:43:56,111 - INFO - Epoch: 50, Training Loss: 0.5014
2025-11-12 01:44:03,834 - INFO - Epoch: 51, Training Loss: 0.4972
2025-11-12 01:44:11,090 - INFO - Epoch: 52, Training Loss: 0.4756
2025-11-12 01:44:18,952 - INFO - Epoch: 53, Training Loss: 0.4837
2025-11-12 01:44:27,124 - INFO - Epoch: 54, Training Loss: 0.4937
2025-11-12 01:44:34,906 - INFO - Epoch: 55, Training Loss: 0.8494
2025-11-12 01:44:42,189 - INFO - Epoch: 56, Training Loss: 0.6470
2025-11-12 01:44:50,409 - INFO - Epoch: 57, Training Loss: 0.6410
2025-11-12 01:44:57,936 - INFO - Epoch: 58, Training Loss: 0.6359
2025-11-12 01:45:06,312 - INFO - Epoch: 59, Training Loss: 0.6362
2025-11-12 01:45:14,541 - INFO - Epoch: 60, Training Loss: 0.6296
2025-11-12 01:45:22,742 - INFO - Epoch: 61, Training Loss: 0.6268
2025-11-12 01:45:30,712 - INFO - Epoch: 62, Training Loss: 0.6255
2025-11-12 01:45:38,231 - INFO - Epoch: 63, Training Loss: 0.6267
2025-11-12 01:45:46,041 - INFO - Epoch: 64, Training Loss: 0.6180
2025-11-12 01:45:53,677 - INFO - Epoch: 65, Training Loss: 0.6152
2025-11-12 01:46:01,988 - INFO - Epoch: 66, Training Loss: 0.6052
2025-11-12 01:46:09,852 - INFO - Epoch: 67, Training Loss: 0.5947
2025-11-12 01:46:18,032 - INFO - Epoch: 68, Training Loss: 0.5886
2025-11-12 01:46:26,536 - INFO - Epoch: 69, Training Loss: 0.5679
2025-11-12 01:46:34,412 - INFO - Epoch: 70, Training Loss: 0.5102
2025-11-12 01:46:41,559 - INFO - Epoch: 71, Training Loss: 0.5049
2025-11-12 01:46:49,561 - INFO - Epoch: 72, Training Loss: 0.7255
2025-11-12 01:46:57,883 - INFO - Epoch: 73, Training Loss: 0.5238
2025-11-12 01:47:05,831 - INFO - Epoch: 74, Training Loss: 0.5237
2025-11-12 01:47:13,526 - INFO - Epoch: 75, Training Loss: 0.6986
2025-11-12 01:47:21,766 - INFO - Epoch: 76, Training Loss: 0.5106
2025-11-12 01:47:29,584 - INFO - Epoch: 77, Training Loss: 0.5104
2025-11-12 01:47:36,934 - INFO - Epoch: 78, Training Loss: 0.4948
2025-11-12 01:47:44,778 - INFO - Epoch: 79, Training Loss: 0.5052
2025-11-12 01:47:52,618 - INFO - Epoch: 80, Training Loss: 0.4878
2025-11-12 01:48:00,364 - INFO - Epoch: 81, Training Loss: 0.4884
2025-11-12 01:48:08,633 - INFO - Epoch: 82, Training Loss: 0.4727
2025-11-12 01:48:17,216 - INFO - Epoch: 83, Training Loss: 0.4673
2025-11-12 01:48:25,033 - INFO - Epoch: 84, Training Loss: 0.4628
2025-11-12 01:48:32,856 - INFO - Epoch: 85, Training Loss: 0.4857
2025-11-12 01:48:40,648 - INFO - Epoch: 86, Training Loss: 0.4802
2025-11-12 01:48:49,674 - INFO - Epoch: 87, Training Loss: 0.4871
2025-11-12 01:48:57,470 - INFO - Epoch: 88, Training Loss: 0.4777
2025-11-12 01:49:05,521 - INFO - Epoch: 89, Training Loss: 0.4867
2025-11-12 01:49:13,353 - INFO - Epoch: 90, Training Loss: 0.4650
2025-11-12 01:49:21,833 - INFO - Epoch: 91, Training Loss: 0.4954
2025-11-12 01:49:30,127 - INFO - Epoch: 92, Training Loss: 0.4645
2025-11-12 01:49:37,594 - INFO - Epoch: 93, Training Loss: 0.5380
2025-11-12 01:49:44,805 - INFO - Epoch: 94, Training Loss: 0.4569
2025-11-12 01:49:52,755 - INFO - Epoch: 95, Training Loss: 0.4573
2025-11-12 01:50:00,741 - INFO - Epoch: 96, Training Loss: 0.4631
2025-11-12 01:50:08,502 - INFO - Epoch: 97, Training Loss: 0.5348
2025-11-12 01:50:16,926 - INFO - Epoch: 98, Training Loss: 0.4559
2025-11-12 01:50:25,016 - INFO - Epoch: 99, Training Loss: 0.4581
2025-11-12 01:50:32,934 - INFO - Epoch: 100, Training Loss: 0.4565
2025-11-12 01:50:32,934 - INFO - Training completed for Trial 1 CV 1

