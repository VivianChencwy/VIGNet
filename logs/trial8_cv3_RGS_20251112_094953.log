2025-11-12 09:49:53,955 - INFO - Log file: ./logs/trial8_cv3_RGS_20251112_094953.log
2025-11-12 09:49:53,955 - INFO - START TRAINING TRIAL 8 CV 3 - Task: RGS
2025-11-12 09:49:53,955 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 09:49:53,955 - INFO - Loading dataset...
2025-11-12 09:49:54,044 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 09:49:54,044 - INFO - Initializing VIGNet model...
2025-11-12 09:49:54,047 - INFO - Number of batch iterations per epoch: 113
2025-11-12 09:50:02,577 - INFO - Epoch: 1, Training Loss: 0.9412
2025-11-12 09:50:10,349 - INFO - Epoch: 2, Training Loss: 0.5759
2025-11-12 09:50:18,255 - INFO - Epoch: 3, Training Loss: 0.5740
2025-11-12 09:50:26,301 - INFO - Epoch: 4, Training Loss: 0.5738
2025-11-12 09:50:34,492 - INFO - Epoch: 5, Training Loss: 0.5722
2025-11-12 09:50:42,124 - INFO - Epoch: 6, Training Loss: 0.5730
2025-11-12 09:50:50,080 - INFO - Epoch: 7, Training Loss: 0.5719
2025-11-12 09:50:57,545 - INFO - Epoch: 8, Training Loss: 0.5714
2025-11-12 09:51:05,095 - INFO - Epoch: 9, Training Loss: 0.5718
2025-11-12 09:51:13,195 - INFO - Epoch: 10, Training Loss: 0.5724
2025-11-12 09:51:20,941 - INFO - Epoch: 11, Training Loss: 0.5717
2025-11-12 09:51:28,834 - INFO - Epoch: 12, Training Loss: 0.5733
2025-11-12 09:51:36,504 - INFO - Epoch: 13, Training Loss: 0.5712
2025-11-12 09:51:44,325 - INFO - Epoch: 14, Training Loss: 0.5705
2025-11-12 09:51:51,535 - INFO - Epoch: 15, Training Loss: 0.5712
2025-11-12 09:52:00,108 - INFO - Epoch: 16, Training Loss: 0.5708
2025-11-12 09:52:08,944 - INFO - Epoch: 17, Training Loss: 0.5690
2025-11-12 09:52:17,053 - INFO - Epoch: 18, Training Loss: 0.5706
2025-11-12 09:52:24,978 - INFO - Epoch: 19, Training Loss: 0.5687
2025-11-12 09:52:32,882 - INFO - Epoch: 20, Training Loss: 0.5711
2025-11-12 09:52:41,700 - INFO - Epoch: 21, Training Loss: 0.5712
2025-11-12 09:52:50,077 - INFO - Epoch: 22, Training Loss: 0.5703
2025-11-12 09:52:58,148 - INFO - Epoch: 23, Training Loss: 0.5697
2025-11-12 09:53:06,530 - INFO - Epoch: 24, Training Loss: 0.5712
2025-11-12 09:53:14,329 - INFO - Epoch: 25, Training Loss: 0.5698
2025-11-12 09:53:23,762 - INFO - Epoch: 26, Training Loss: 0.5708
2025-11-12 09:53:31,515 - INFO - Epoch: 27, Training Loss: 0.5701
2025-11-12 09:53:40,073 - INFO - Epoch: 28, Training Loss: 0.5709
2025-11-12 09:53:47,680 - INFO - Epoch: 29, Training Loss: 0.5694
2025-11-12 09:53:55,543 - INFO - Epoch: 30, Training Loss: 0.5696
2025-11-12 09:54:03,771 - INFO - Epoch: 31, Training Loss: 0.5656
2025-11-12 09:54:12,132 - INFO - Epoch: 32, Training Loss: 0.5546
2025-11-12 09:54:19,638 - INFO - Epoch: 33, Training Loss: 0.5361
2025-11-12 09:54:26,962 - INFO - Epoch: 34, Training Loss: 0.5228
2025-11-12 09:54:35,704 - INFO - Epoch: 35, Training Loss: 0.5030
2025-11-12 09:54:43,776 - INFO - Epoch: 36, Training Loss: 0.5329
2025-11-12 09:54:51,954 - INFO - Epoch: 37, Training Loss: 0.5232
2025-11-12 09:54:59,449 - INFO - Epoch: 38, Training Loss: 0.4953
2025-11-12 09:55:07,665 - INFO - Epoch: 39, Training Loss: 0.4826
2025-11-12 09:55:15,212 - INFO - Epoch: 40, Training Loss: 0.4833
2025-11-12 09:55:22,914 - INFO - Epoch: 41, Training Loss: 0.4821
2025-11-12 09:55:31,641 - INFO - Epoch: 42, Training Loss: 0.4759
2025-11-12 09:55:39,739 - INFO - Epoch: 43, Training Loss: 0.4827
2025-11-12 09:55:47,739 - INFO - Epoch: 44, Training Loss: 0.4663
2025-11-12 09:55:55,111 - INFO - Epoch: 45, Training Loss: 0.4680
2025-11-12 09:56:03,422 - INFO - Epoch: 46, Training Loss: 0.4676
2025-11-12 09:56:11,024 - INFO - Epoch: 47, Training Loss: 0.4874
2025-11-12 09:56:19,496 - INFO - Epoch: 48, Training Loss: 0.4661
2025-11-12 09:56:28,208 - INFO - Epoch: 49, Training Loss: 0.4701
2025-11-12 09:56:36,350 - INFO - Epoch: 50, Training Loss: 0.4686
2025-11-12 09:56:44,637 - INFO - Epoch: 51, Training Loss: 0.4791
2025-11-12 09:56:52,273 - INFO - Epoch: 52, Training Loss: 0.4659
2025-11-12 09:56:59,867 - INFO - Epoch: 53, Training Loss: 0.4718
2025-11-12 09:57:07,713 - INFO - Epoch: 54, Training Loss: 0.4730
2025-11-12 09:57:15,885 - INFO - Epoch: 55, Training Loss: 0.4689
2025-11-12 09:57:23,422 - INFO - Epoch: 56, Training Loss: 0.4618
2025-11-12 09:57:31,183 - INFO - Epoch: 57, Training Loss: 0.4632
2025-11-12 09:57:39,492 - INFO - Epoch: 58, Training Loss: 0.4701
2025-11-12 09:57:47,210 - INFO - Epoch: 59, Training Loss: 0.4606
2025-11-12 09:57:54,946 - INFO - Epoch: 60, Training Loss: 0.4635
2025-11-12 09:58:03,021 - INFO - Epoch: 61, Training Loss: 0.4664
2025-11-12 09:58:10,275 - INFO - Epoch: 62, Training Loss: 0.4643
2025-11-12 09:58:18,059 - INFO - Epoch: 63, Training Loss: 0.4890
2025-11-12 09:58:26,615 - INFO - Epoch: 64, Training Loss: 0.4643
2025-11-12 09:58:35,050 - INFO - Epoch: 65, Training Loss: 0.4663
2025-11-12 09:58:43,151 - INFO - Epoch: 66, Training Loss: 0.4588
2025-11-12 09:58:51,212 - INFO - Epoch: 67, Training Loss: 0.4580
2025-11-12 09:58:58,658 - INFO - Epoch: 68, Training Loss: 0.4591
2025-11-12 09:59:06,842 - INFO - Epoch: 69, Training Loss: 0.4568
2025-11-12 09:59:14,398 - INFO - Epoch: 70, Training Loss: 0.4568
2025-11-12 09:59:22,420 - INFO - Epoch: 71, Training Loss: 0.4572
2025-11-12 09:59:30,519 - INFO - Epoch: 72, Training Loss: 0.4568
2025-11-12 09:59:37,989 - INFO - Epoch: 73, Training Loss: 0.5369
2025-11-12 09:59:46,073 - INFO - Epoch: 74, Training Loss: 0.5670
2025-11-12 09:59:53,401 - INFO - Epoch: 75, Training Loss: 0.5673
2025-11-12 10:00:01,034 - INFO - Epoch: 76, Training Loss: 0.5646
2025-11-12 10:00:08,989 - INFO - Epoch: 77, Training Loss: 0.5627
2025-11-12 10:00:17,467 - INFO - Epoch: 78, Training Loss: 0.5564
2025-11-12 10:00:25,280 - INFO - Epoch: 79, Training Loss: 0.5554
2025-11-12 10:00:33,699 - INFO - Epoch: 80, Training Loss: 0.5310
2025-11-12 10:00:41,353 - INFO - Epoch: 81, Training Loss: 0.5327
2025-11-12 10:00:48,563 - INFO - Epoch: 82, Training Loss: 0.5067
2025-11-12 10:00:57,250 - INFO - Epoch: 83, Training Loss: 0.5475
2025-11-12 10:01:05,121 - INFO - Epoch: 84, Training Loss: 0.5550
2025-11-12 10:01:12,828 - INFO - Epoch: 85, Training Loss: 0.5509
2025-11-12 10:01:20,398 - INFO - Epoch: 86, Training Loss: 0.5522
2025-11-12 10:01:27,927 - INFO - Epoch: 87, Training Loss: 0.5459
2025-11-12 10:01:36,162 - INFO - Epoch: 88, Training Loss: 0.5470
2025-11-12 10:01:44,751 - INFO - Epoch: 89, Training Loss: 0.5395
2025-11-12 10:01:52,360 - INFO - Epoch: 90, Training Loss: 0.5343
2025-11-12 10:02:00,951 - INFO - Epoch: 91, Training Loss: 0.5228
2025-11-12 10:02:08,953 - INFO - Epoch: 92, Training Loss: 0.5139
2025-11-12 10:02:16,578 - INFO - Epoch: 93, Training Loss: 0.4875
2025-11-12 10:02:23,742 - INFO - Epoch: 94, Training Loss: 0.4881
2025-11-12 10:02:30,820 - INFO - Epoch: 95, Training Loss: 0.4772
2025-11-12 10:02:38,489 - INFO - Epoch: 96, Training Loss: 0.4697
2025-11-12 10:02:46,072 - INFO - Epoch: 97, Training Loss: 0.4768
2025-11-12 10:02:54,234 - INFO - Epoch: 98, Training Loss: 0.4676
2025-11-12 10:03:01,253 - INFO - Epoch: 99, Training Loss: 0.4737
2025-11-12 10:03:09,189 - INFO - Epoch: 100, Training Loss: 0.4713
2025-11-12 10:03:09,190 - INFO - Training completed for Trial 8 CV 3

