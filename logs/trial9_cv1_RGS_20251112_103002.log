2025-11-12 10:30:02,499 - INFO - Log file: ./logs/trial9_cv1_RGS_20251112_103002.log
2025-11-12 10:30:02,499 - INFO - START TRAINING TRIAL 9 CV 1 - Task: RGS
2025-11-12 10:30:02,499 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 10:30:02,499 - INFO - Loading dataset...
2025-11-12 10:30:02,633 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 10:30:02,633 - INFO - Initializing VIGNet model...
2025-11-12 10:30:02,636 - INFO - Number of batch iterations per epoch: 113
2025-11-12 10:30:10,711 - INFO - Epoch: 1, Training Loss: 1.0620
2025-11-12 10:30:18,987 - INFO - Epoch: 2, Training Loss: 0.7427
2025-11-12 10:30:27,660 - INFO - Epoch: 3, Training Loss: 0.6965
2025-11-12 10:30:35,650 - INFO - Epoch: 4, Training Loss: 0.6909
2025-11-12 10:30:43,607 - INFO - Epoch: 5, Training Loss: 0.6907
2025-11-12 10:30:51,651 - INFO - Epoch: 6, Training Loss: 0.6886
2025-11-12 10:30:59,089 - INFO - Epoch: 7, Training Loss: 0.6885
2025-11-12 10:31:06,820 - INFO - Epoch: 8, Training Loss: 0.6881
2025-11-12 10:31:14,905 - INFO - Epoch: 9, Training Loss: 0.6876
2025-11-12 10:31:23,298 - INFO - Epoch: 10, Training Loss: 0.6885
2025-11-12 10:31:31,733 - INFO - Epoch: 11, Training Loss: 0.6874
2025-11-12 10:31:39,543 - INFO - Epoch: 12, Training Loss: 0.6871
2025-11-12 10:31:47,276 - INFO - Epoch: 13, Training Loss: 0.6871
2025-11-12 10:31:54,786 - INFO - Epoch: 14, Training Loss: 0.6865
2025-11-12 10:32:03,294 - INFO - Epoch: 15, Training Loss: 0.6889
2025-11-12 10:32:11,414 - INFO - Epoch: 16, Training Loss: 0.6873
2025-11-12 10:32:19,392 - INFO - Epoch: 17, Training Loss: 0.6869
2025-11-12 10:32:27,130 - INFO - Epoch: 18, Training Loss: 0.6865
2025-11-12 10:32:34,732 - INFO - Epoch: 19, Training Loss: 0.6874
2025-11-12 10:32:43,254 - INFO - Epoch: 20, Training Loss: 0.6865
2025-11-12 10:32:51,326 - INFO - Epoch: 21, Training Loss: 0.6887
2025-11-12 10:32:59,625 - INFO - Epoch: 22, Training Loss: 0.6877
2025-11-12 10:33:08,835 - INFO - Epoch: 23, Training Loss: 0.6866
2025-11-12 10:33:16,684 - INFO - Epoch: 24, Training Loss: 0.6873
2025-11-12 10:33:24,385 - INFO - Epoch: 25, Training Loss: 0.6872
2025-11-12 10:33:32,697 - INFO - Epoch: 26, Training Loss: 0.6875
2025-11-12 10:33:40,731 - INFO - Epoch: 27, Training Loss: 0.6864
2025-11-12 10:33:48,861 - INFO - Epoch: 28, Training Loss: 0.6865
2025-11-12 10:33:56,641 - INFO - Epoch: 29, Training Loss: 0.6871
2025-11-12 10:34:04,283 - INFO - Epoch: 30, Training Loss: 0.6874
2025-11-12 10:34:12,648 - INFO - Epoch: 31, Training Loss: 0.6870
2025-11-12 10:34:20,685 - INFO - Epoch: 32, Training Loss: 0.6866
2025-11-12 10:34:29,020 - INFO - Epoch: 33, Training Loss: 0.6877
2025-11-12 10:34:37,436 - INFO - Epoch: 34, Training Loss: 0.6873
2025-11-12 10:34:45,789 - INFO - Epoch: 35, Training Loss: 0.6874
2025-11-12 10:34:54,240 - INFO - Epoch: 36, Training Loss: 0.6860
2025-11-12 10:35:02,676 - INFO - Epoch: 37, Training Loss: 0.6882
2025-11-12 10:35:10,448 - INFO - Epoch: 38, Training Loss: 0.6863
2025-11-12 10:35:18,251 - INFO - Epoch: 39, Training Loss: 0.6869
2025-11-12 10:35:26,305 - INFO - Epoch: 40, Training Loss: 0.6862
2025-11-12 10:35:33,731 - INFO - Epoch: 41, Training Loss: 0.6874
2025-11-12 10:35:41,831 - INFO - Epoch: 42, Training Loss: 0.6874
2025-11-12 10:35:49,766 - INFO - Epoch: 43, Training Loss: 0.6862
2025-11-12 10:35:57,837 - INFO - Epoch: 44, Training Loss: 0.6864
2025-11-12 10:36:05,448 - INFO - Epoch: 45, Training Loss: 0.6864
2025-11-12 10:36:13,706 - INFO - Epoch: 46, Training Loss: 0.6862
2025-11-12 10:36:21,128 - INFO - Epoch: 47, Training Loss: 0.6871
2025-11-12 10:36:29,112 - INFO - Epoch: 48, Training Loss: 0.6845
2025-11-12 10:36:36,536 - INFO - Epoch: 49, Training Loss: 0.6841
2025-11-12 10:36:44,025 - INFO - Epoch: 50, Training Loss: 0.6808
2025-11-12 10:36:51,937 - INFO - Epoch: 51, Training Loss: 0.6778
2025-11-12 10:36:59,727 - INFO - Epoch: 52, Training Loss: 0.6737
2025-11-12 10:37:07,798 - INFO - Epoch: 53, Training Loss: 0.6740
2025-11-12 10:37:15,888 - INFO - Epoch: 54, Training Loss: 0.6730
2025-11-12 10:37:23,447 - INFO - Epoch: 55, Training Loss: 0.6731
2025-11-12 10:37:31,346 - INFO - Epoch: 56, Training Loss: 0.6714
2025-11-12 10:37:39,025 - INFO - Epoch: 57, Training Loss: 0.6709
2025-11-12 10:37:47,040 - INFO - Epoch: 58, Training Loss: 0.6717
2025-11-12 10:37:55,547 - INFO - Epoch: 59, Training Loss: 0.6703
2025-11-12 10:38:03,541 - INFO - Epoch: 60, Training Loss: 0.6711
2025-11-12 10:38:11,402 - INFO - Epoch: 61, Training Loss: 0.6707
2025-11-12 10:38:19,408 - INFO - Epoch: 62, Training Loss: 0.6712
2025-11-12 10:38:26,850 - INFO - Epoch: 63, Training Loss: 0.6707
2025-11-12 10:38:34,581 - INFO - Epoch: 64, Training Loss: 0.6710
2025-11-12 10:38:42,910 - INFO - Epoch: 65, Training Loss: 0.6698
2025-11-12 10:38:50,812 - INFO - Epoch: 66, Training Loss: 0.6706
2025-11-12 10:38:59,367 - INFO - Epoch: 67, Training Loss: 0.6712
2025-11-12 10:39:07,020 - INFO - Epoch: 68, Training Loss: 0.6702
2025-11-12 10:39:14,942 - INFO - Epoch: 69, Training Loss: 0.6731
2025-11-12 10:39:23,162 - INFO - Epoch: 70, Training Loss: 0.6702
2025-11-12 10:39:31,268 - INFO - Epoch: 71, Training Loss: 0.6700
2025-11-12 10:39:39,617 - INFO - Epoch: 72, Training Loss: 0.6699
2025-11-12 10:39:47,455 - INFO - Epoch: 73, Training Loss: 0.6699
2025-11-12 10:39:55,771 - INFO - Epoch: 74, Training Loss: 0.6709
2025-11-12 10:40:03,685 - INFO - Epoch: 75, Training Loss: 0.6692
2025-11-12 10:40:11,844 - INFO - Epoch: 76, Training Loss: 0.6704
2025-11-12 10:40:20,029 - INFO - Epoch: 77, Training Loss: 0.6703
2025-11-12 10:40:28,691 - INFO - Epoch: 78, Training Loss: 0.6718
2025-11-12 10:40:36,323 - INFO - Epoch: 79, Training Loss: 0.6696
2025-11-12 10:40:43,671 - INFO - Epoch: 80, Training Loss: 0.6705
2025-11-12 10:40:51,625 - INFO - Epoch: 81, Training Loss: 0.6697
2025-11-12 10:40:59,972 - INFO - Epoch: 82, Training Loss: 0.6706
2025-11-12 10:41:07,689 - INFO - Epoch: 83, Training Loss: 0.6703
2025-11-12 10:41:15,940 - INFO - Epoch: 84, Training Loss: 0.6701
2025-11-12 10:41:23,520 - INFO - Epoch: 85, Training Loss: 0.6706
2025-11-12 10:41:32,152 - INFO - Epoch: 86, Training Loss: 0.6698
2025-11-12 10:41:40,410 - INFO - Epoch: 87, Training Loss: 0.6694
2025-11-12 10:41:49,708 - INFO - Epoch: 88, Training Loss: 0.6760
2025-11-12 10:41:58,825 - INFO - Epoch: 89, Training Loss: 0.6712
2025-11-12 10:42:07,974 - INFO - Epoch: 90, Training Loss: 0.6714
2025-11-12 10:42:15,539 - INFO - Epoch: 91, Training Loss: 0.6693
2025-11-12 10:42:23,578 - INFO - Epoch: 92, Training Loss: 0.6699
2025-11-12 10:42:31,573 - INFO - Epoch: 93, Training Loss: 0.6697
2025-11-12 10:42:39,414 - INFO - Epoch: 94, Training Loss: 0.6700
2025-11-12 10:42:47,268 - INFO - Epoch: 95, Training Loss: 0.6697
2025-11-12 10:42:54,439 - INFO - Epoch: 96, Training Loss: 0.6696
2025-11-12 10:43:02,400 - INFO - Epoch: 97, Training Loss: 0.6689
2025-11-12 10:43:11,450 - INFO - Epoch: 98, Training Loss: 0.6686
2025-11-12 10:43:19,559 - INFO - Epoch: 99, Training Loss: 0.6684
2025-11-12 10:43:27,434 - INFO - Epoch: 100, Training Loss: 0.6694
2025-11-12 10:43:27,434 - INFO - Training completed for Trial 9 CV 1

