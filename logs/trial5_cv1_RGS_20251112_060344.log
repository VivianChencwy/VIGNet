2025-11-12 06:03:44,602 - INFO - Log file: ./logs/trial5_cv1_RGS_20251112_060344.log
2025-11-12 06:03:44,603 - INFO - START TRAINING TRIAL 5 CV 1 - Task: RGS
2025-11-12 06:03:44,603 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 06:03:44,603 - INFO - Loading dataset...
2025-11-12 06:03:44,721 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 06:03:44,721 - INFO - Initializing VIGNet model...
2025-11-12 06:03:44,724 - INFO - Number of batch iterations per epoch: 113
2025-11-12 06:03:52,533 - INFO - Epoch: 1, Training Loss: 1.3693
2025-11-12 06:03:59,914 - INFO - Epoch: 2, Training Loss: 0.7144
2025-11-12 06:04:07,643 - INFO - Epoch: 3, Training Loss: 0.7038
2025-11-12 06:04:15,429 - INFO - Epoch: 4, Training Loss: 0.6985
2025-11-12 06:04:23,607 - INFO - Epoch: 5, Training Loss: 0.6946
2025-11-12 06:04:30,812 - INFO - Epoch: 6, Training Loss: 0.6926
2025-11-12 06:04:37,903 - INFO - Epoch: 7, Training Loss: 0.6903
2025-11-12 06:04:45,593 - INFO - Epoch: 8, Training Loss: 0.6884
2025-11-12 06:04:53,739 - INFO - Epoch: 9, Training Loss: 0.6936
2025-11-12 06:05:01,134 - INFO - Epoch: 10, Training Loss: 0.6904
2025-11-12 06:05:08,565 - INFO - Epoch: 11, Training Loss: 0.6935
2025-11-12 06:05:16,505 - INFO - Epoch: 12, Training Loss: 0.6913
2025-11-12 06:05:23,714 - INFO - Epoch: 13, Training Loss: 0.6907
2025-11-12 06:05:31,624 - INFO - Epoch: 14, Training Loss: 0.6912
2025-11-12 06:05:38,905 - INFO - Epoch: 15, Training Loss: 0.6917
2025-11-12 06:05:46,764 - INFO - Epoch: 16, Training Loss: 0.6889
2025-11-12 06:05:54,979 - INFO - Epoch: 17, Training Loss: 0.6939
2025-11-12 06:06:04,010 - INFO - Epoch: 18, Training Loss: 0.6885
2025-11-12 06:06:11,623 - INFO - Epoch: 19, Training Loss: 0.6885
2025-11-12 06:06:19,750 - INFO - Epoch: 20, Training Loss: 0.6889
2025-11-12 06:06:28,279 - INFO - Epoch: 21, Training Loss: 0.6905
2025-11-12 06:06:35,908 - INFO - Epoch: 22, Training Loss: 0.6895
2025-11-12 06:06:43,758 - INFO - Epoch: 23, Training Loss: 0.6904
2025-11-12 06:06:51,338 - INFO - Epoch: 24, Training Loss: 0.6905
2025-11-12 06:06:58,555 - INFO - Epoch: 25, Training Loss: 0.6889
2025-11-12 06:07:06,841 - INFO - Epoch: 26, Training Loss: 0.6910
2025-11-12 06:07:14,345 - INFO - Epoch: 27, Training Loss: 0.6912
2025-11-12 06:07:22,194 - INFO - Epoch: 28, Training Loss: 0.6935
2025-11-12 06:07:30,413 - INFO - Epoch: 29, Training Loss: 0.6897
2025-11-12 06:07:38,597 - INFO - Epoch: 30, Training Loss: 0.6922
2025-11-12 06:07:46,304 - INFO - Epoch: 31, Training Loss: 0.6921
2025-11-12 06:07:54,113 - INFO - Epoch: 32, Training Loss: 0.6889
2025-11-12 06:08:01,839 - INFO - Epoch: 33, Training Loss: 0.6896
2025-11-12 06:08:09,294 - INFO - Epoch: 34, Training Loss: 0.6895
2025-11-12 06:08:17,841 - INFO - Epoch: 35, Training Loss: 0.6891
2025-11-12 06:08:26,605 - INFO - Epoch: 36, Training Loss: 0.6908
2025-11-12 06:08:34,020 - INFO - Epoch: 37, Training Loss: 0.6902
2025-11-12 06:08:41,503 - INFO - Epoch: 38, Training Loss: 0.6879
2025-11-12 06:08:48,887 - INFO - Epoch: 39, Training Loss: 0.6879
2025-11-12 06:08:56,970 - INFO - Epoch: 40, Training Loss: 0.6894
2025-11-12 06:09:05,113 - INFO - Epoch: 41, Training Loss: 0.6890
2025-11-12 06:09:13,354 - INFO - Epoch: 42, Training Loss: 0.6863
2025-11-12 06:09:21,421 - INFO - Epoch: 43, Training Loss: 0.6863
2025-11-12 06:09:29,340 - INFO - Epoch: 44, Training Loss: 0.6810
2025-11-12 06:09:37,842 - INFO - Epoch: 45, Training Loss: 0.6682
2025-11-12 06:09:45,564 - INFO - Epoch: 46, Training Loss: 0.6507
2025-11-12 06:09:54,034 - INFO - Epoch: 47, Training Loss: 0.6374
2025-11-12 06:10:01,746 - INFO - Epoch: 48, Training Loss: 0.6419
2025-11-12 06:10:10,489 - INFO - Epoch: 49, Training Loss: 0.6198
2025-11-12 06:10:18,162 - INFO - Epoch: 50, Training Loss: 0.7043
2025-11-12 06:10:26,149 - INFO - Epoch: 51, Training Loss: 0.6832
2025-11-12 06:10:34,540 - INFO - Epoch: 52, Training Loss: 0.6646
2025-11-12 06:10:41,989 - INFO - Epoch: 53, Training Loss: 0.6562
2025-11-12 06:10:50,059 - INFO - Epoch: 54, Training Loss: 0.6468
2025-11-12 06:10:57,568 - INFO - Epoch: 55, Training Loss: 0.6502
2025-11-12 06:11:06,004 - INFO - Epoch: 56, Training Loss: 0.6433
2025-11-12 06:11:13,608 - INFO - Epoch: 57, Training Loss: 0.6450
2025-11-12 06:11:21,785 - INFO - Epoch: 58, Training Loss: 0.6454
2025-11-12 06:11:29,371 - INFO - Epoch: 59, Training Loss: 0.6616
2025-11-12 06:11:36,643 - INFO - Epoch: 60, Training Loss: 0.6383
2025-11-12 06:11:44,974 - INFO - Epoch: 61, Training Loss: 0.6337
2025-11-12 06:11:53,150 - INFO - Epoch: 62, Training Loss: 0.6318
2025-11-12 06:12:01,377 - INFO - Epoch: 63, Training Loss: 0.6260
2025-11-12 06:12:09,883 - INFO - Epoch: 64, Training Loss: 0.6250
2025-11-12 06:12:18,172 - INFO - Epoch: 65, Training Loss: 0.6212
2025-11-12 06:12:26,334 - INFO - Epoch: 66, Training Loss: 0.6281
2025-11-12 06:12:33,951 - INFO - Epoch: 67, Training Loss: 0.6216
2025-11-12 06:12:41,330 - INFO - Epoch: 68, Training Loss: 0.6217
2025-11-12 06:12:48,969 - INFO - Epoch: 69, Training Loss: 0.6204
2025-11-12 06:12:57,486 - INFO - Epoch: 70, Training Loss: 0.6151
2025-11-12 06:13:05,216 - INFO - Epoch: 71, Training Loss: 0.6315
2025-11-12 06:13:13,222 - INFO - Epoch: 72, Training Loss: 0.6113
2025-11-12 06:13:21,867 - INFO - Epoch: 73, Training Loss: 0.6280
2025-11-12 06:13:29,403 - INFO - Epoch: 74, Training Loss: 0.6106
2025-11-12 06:13:38,006 - INFO - Epoch: 75, Training Loss: 0.6024
2025-11-12 06:13:45,889 - INFO - Epoch: 76, Training Loss: 0.6107
2025-11-12 06:13:53,349 - INFO - Epoch: 77, Training Loss: 0.6027
2025-11-12 06:14:01,114 - INFO - Epoch: 78, Training Loss: 0.5852
2025-11-12 06:14:09,403 - INFO - Epoch: 79, Training Loss: 0.5827
2025-11-12 06:14:17,255 - INFO - Epoch: 80, Training Loss: 0.5792
2025-11-12 06:14:25,146 - INFO - Epoch: 81, Training Loss: 0.5783
2025-11-12 06:14:33,388 - INFO - Epoch: 82, Training Loss: 0.5618
2025-11-12 06:14:42,290 - INFO - Epoch: 83, Training Loss: 0.5790
2025-11-12 06:14:50,327 - INFO - Epoch: 84, Training Loss: 0.9811
2025-11-12 06:14:58,838 - INFO - Epoch: 85, Training Loss: 0.6173
2025-11-12 06:15:06,339 - INFO - Epoch: 86, Training Loss: 0.5985
2025-11-12 06:15:13,733 - INFO - Epoch: 87, Training Loss: 0.5974
2025-11-12 06:15:21,321 - INFO - Epoch: 88, Training Loss: 0.5928
2025-11-12 06:15:28,475 - INFO - Epoch: 89, Training Loss: 0.6001
2025-11-12 06:15:36,585 - INFO - Epoch: 90, Training Loss: 0.6072
2025-11-12 06:15:44,302 - INFO - Epoch: 91, Training Loss: 0.5903
2025-11-12 06:15:51,672 - INFO - Epoch: 92, Training Loss: 0.5935
2025-11-12 06:15:59,628 - INFO - Epoch: 93, Training Loss: 0.5821
2025-11-12 06:16:07,481 - INFO - Epoch: 94, Training Loss: 0.5758
2025-11-12 06:16:14,887 - INFO - Epoch: 95, Training Loss: 0.5813
2025-11-12 06:16:22,660 - INFO - Epoch: 96, Training Loss: 0.5884
2025-11-12 06:16:30,663 - INFO - Epoch: 97, Training Loss: 0.5823
2025-11-12 06:16:39,201 - INFO - Epoch: 98, Training Loss: 0.5740
2025-11-12 06:16:47,045 - INFO - Epoch: 99, Training Loss: 0.5693
2025-11-12 06:16:55,203 - INFO - Epoch: 100, Training Loss: 0.6006
2025-11-12 06:16:55,203 - INFO - Training completed for Trial 5 CV 1

