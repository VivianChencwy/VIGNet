2025-11-12 09:36:32,011 - INFO - Log file: ./logs/trial8_cv2_RGS_20251112_093632.log
2025-11-12 09:36:32,012 - INFO - START TRAINING TRIAL 8 CV 2 - Task: RGS
2025-11-12 09:36:32,012 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 09:36:32,012 - INFO - Loading dataset...
2025-11-12 09:36:32,105 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 09:36:32,105 - INFO - Initializing VIGNet model...
2025-11-12 09:36:32,109 - INFO - Number of batch iterations per epoch: 113
2025-11-12 09:36:39,735 - INFO - Epoch: 1, Training Loss: 1.1707
2025-11-12 09:36:48,115 - INFO - Epoch: 2, Training Loss: 0.6275
2025-11-12 09:36:56,100 - INFO - Epoch: 3, Training Loss: 0.5989
2025-11-12 09:37:03,957 - INFO - Epoch: 4, Training Loss: 0.5948
2025-11-12 09:37:11,643 - INFO - Epoch: 5, Training Loss: 0.5951
2025-11-12 09:37:19,832 - INFO - Epoch: 6, Training Loss: 0.5936
2025-11-12 09:37:28,412 - INFO - Epoch: 7, Training Loss: 0.5913
2025-11-12 09:37:37,104 - INFO - Epoch: 8, Training Loss: 0.5947
2025-11-12 09:37:45,675 - INFO - Epoch: 9, Training Loss: 0.5933
2025-11-12 09:37:53,545 - INFO - Epoch: 10, Training Loss: 0.5911
2025-11-12 09:38:01,415 - INFO - Epoch: 11, Training Loss: 0.5918
2025-11-12 09:38:08,890 - INFO - Epoch: 12, Training Loss: 0.5903
2025-11-12 09:38:16,970 - INFO - Epoch: 13, Training Loss: 0.5913
2025-11-12 09:38:25,224 - INFO - Epoch: 14, Training Loss: 0.5903
2025-11-12 09:38:32,802 - INFO - Epoch: 15, Training Loss: 0.5909
2025-11-12 09:38:40,550 - INFO - Epoch: 16, Training Loss: 0.5897
2025-11-12 09:38:49,017 - INFO - Epoch: 17, Training Loss: 0.5924
2025-11-12 09:38:57,999 - INFO - Epoch: 18, Training Loss: 0.5917
2025-11-12 09:39:05,985 - INFO - Epoch: 19, Training Loss: 0.5897
2025-11-12 09:39:13,615 - INFO - Epoch: 20, Training Loss: 0.5917
2025-11-12 09:39:21,134 - INFO - Epoch: 21, Training Loss: 0.5905
2025-11-12 09:39:28,711 - INFO - Epoch: 22, Training Loss: 0.5901
2025-11-12 09:39:37,284 - INFO - Epoch: 23, Training Loss: 0.5894
2025-11-12 09:39:45,716 - INFO - Epoch: 24, Training Loss: 0.5896
2025-11-12 09:39:53,126 - INFO - Epoch: 25, Training Loss: 0.5888
2025-11-12 09:40:01,670 - INFO - Epoch: 26, Training Loss: 0.5906
2025-11-12 09:40:10,196 - INFO - Epoch: 27, Training Loss: 0.5886
2025-11-12 09:40:18,315 - INFO - Epoch: 28, Training Loss: 0.5854
2025-11-12 09:40:26,130 - INFO - Epoch: 29, Training Loss: 0.5842
2025-11-12 09:40:33,882 - INFO - Epoch: 30, Training Loss: 0.5688
2025-11-12 09:40:41,623 - INFO - Epoch: 31, Training Loss: 0.5424
2025-11-12 09:40:50,079 - INFO - Epoch: 32, Training Loss: 0.5392
2025-11-12 09:40:58,442 - INFO - Epoch: 33, Training Loss: 0.5175
2025-11-12 09:41:06,730 - INFO - Epoch: 34, Training Loss: 0.5370
2025-11-12 09:41:14,495 - INFO - Epoch: 35, Training Loss: 0.5055
2025-11-12 09:41:22,787 - INFO - Epoch: 36, Training Loss: 0.6205
2025-11-12 09:41:30,754 - INFO - Epoch: 37, Training Loss: 0.5835
2025-11-12 09:41:38,127 - INFO - Epoch: 38, Training Loss: 0.5839
2025-11-12 09:41:46,376 - INFO - Epoch: 39, Training Loss: 0.5848
2025-11-12 09:41:54,583 - INFO - Epoch: 40, Training Loss: 0.5827
2025-11-12 09:42:02,169 - INFO - Epoch: 41, Training Loss: 0.5823
2025-11-12 09:42:11,239 - INFO - Epoch: 42, Training Loss: 0.5848
2025-11-12 09:42:19,751 - INFO - Epoch: 43, Training Loss: 0.5785
2025-11-12 09:42:28,327 - INFO - Epoch: 44, Training Loss: 0.5799
2025-11-12 09:42:36,550 - INFO - Epoch: 45, Training Loss: 0.5794
2025-11-12 09:42:44,525 - INFO - Epoch: 46, Training Loss: 0.5355
2025-11-12 09:42:52,171 - INFO - Epoch: 47, Training Loss: 0.5240
2025-11-12 09:42:59,987 - INFO - Epoch: 48, Training Loss: 0.5001
2025-11-12 09:43:08,149 - INFO - Epoch: 49, Training Loss: 0.5058
2025-11-12 09:43:16,420 - INFO - Epoch: 50, Training Loss: 0.5975
2025-11-12 09:43:24,674 - INFO - Epoch: 51, Training Loss: 0.5493
2025-11-12 09:43:33,250 - INFO - Epoch: 52, Training Loss: 0.5349
2025-11-12 09:43:40,806 - INFO - Epoch: 53, Training Loss: 0.5288
2025-11-12 09:43:48,315 - INFO - Epoch: 54, Training Loss: 0.5145
2025-11-12 09:43:56,430 - INFO - Epoch: 55, Training Loss: 0.5026
2025-11-12 09:44:04,309 - INFO - Epoch: 56, Training Loss: 0.5084
2025-11-12 09:44:12,059 - INFO - Epoch: 57, Training Loss: 0.5105
2025-11-12 09:44:19,697 - INFO - Epoch: 58, Training Loss: 0.4982
2025-11-12 09:44:27,936 - INFO - Epoch: 59, Training Loss: 0.5084
2025-11-12 09:44:35,650 - INFO - Epoch: 60, Training Loss: 0.4898
2025-11-12 09:44:44,212 - INFO - Epoch: 61, Training Loss: 0.5169
2025-11-12 09:44:52,178 - INFO - Epoch: 62, Training Loss: 0.4910
2025-11-12 09:44:59,706 - INFO - Epoch: 63, Training Loss: 0.5110
2025-11-12 09:45:07,427 - INFO - Epoch: 64, Training Loss: 0.5173
2025-11-12 09:45:15,632 - INFO - Epoch: 65, Training Loss: 0.4904
2025-11-12 09:45:23,561 - INFO - Epoch: 66, Training Loss: 0.5099
2025-11-12 09:45:31,123 - INFO - Epoch: 67, Training Loss: 0.4792
2025-11-12 09:45:38,598 - INFO - Epoch: 68, Training Loss: 0.4814
2025-11-12 09:45:46,884 - INFO - Epoch: 69, Training Loss: 0.4786
2025-11-12 09:45:54,661 - INFO - Epoch: 70, Training Loss: 0.4775
2025-11-12 09:46:02,420 - INFO - Epoch: 71, Training Loss: 0.4800
2025-11-12 09:46:09,618 - INFO - Epoch: 72, Training Loss: 0.4740
2025-11-12 09:46:17,314 - INFO - Epoch: 73, Training Loss: 0.4758
2025-11-12 09:46:24,986 - INFO - Epoch: 74, Training Loss: 0.4963
2025-11-12 09:46:33,091 - INFO - Epoch: 75, Training Loss: 0.4781
2025-11-12 09:46:41,491 - INFO - Epoch: 76, Training Loss: 0.4857
2025-11-12 09:46:49,599 - INFO - Epoch: 77, Training Loss: 0.4835
2025-11-12 09:46:57,694 - INFO - Epoch: 78, Training Loss: 0.4821
2025-11-12 09:47:07,412 - INFO - Epoch: 79, Training Loss: 0.4733
2025-11-12 09:47:15,656 - INFO - Epoch: 80, Training Loss: 0.4760
2025-11-12 09:47:23,156 - INFO - Epoch: 81, Training Loss: 0.4719
2025-11-12 09:47:31,398 - INFO - Epoch: 82, Training Loss: 0.4701
2025-11-12 09:47:39,972 - INFO - Epoch: 83, Training Loss: 0.4770
2025-11-12 09:47:48,257 - INFO - Epoch: 84, Training Loss: 0.4739
2025-11-12 09:47:55,952 - INFO - Epoch: 85, Training Loss: 0.4736
2025-11-12 09:48:04,108 - INFO - Epoch: 86, Training Loss: 0.4776
2025-11-12 09:48:11,964 - INFO - Epoch: 87, Training Loss: 0.4746
2025-11-12 09:48:19,284 - INFO - Epoch: 88, Training Loss: 0.4727
2025-11-12 09:48:26,812 - INFO - Epoch: 89, Training Loss: 0.4648
2025-11-12 09:48:34,328 - INFO - Epoch: 90, Training Loss: 0.4751
2025-11-12 09:48:42,200 - INFO - Epoch: 91, Training Loss: 0.4682
2025-11-12 09:48:50,056 - INFO - Epoch: 92, Training Loss: 0.4696
2025-11-12 09:48:58,104 - INFO - Epoch: 93, Training Loss: 0.4779
2025-11-12 09:49:05,987 - INFO - Epoch: 94, Training Loss: 0.4755
2025-11-12 09:49:13,847 - INFO - Epoch: 95, Training Loss: 0.4662
2025-11-12 09:49:21,824 - INFO - Epoch: 96, Training Loss: 0.4851
2025-11-12 09:49:29,960 - INFO - Epoch: 97, Training Loss: 0.4805
2025-11-12 09:49:38,263 - INFO - Epoch: 98, Training Loss: 0.4677
2025-11-12 09:49:45,538 - INFO - Epoch: 99, Training Loss: 0.4635
2025-11-12 09:49:53,954 - INFO - Epoch: 100, Training Loss: 0.4650
2025-11-12 09:49:53,954 - INFO - Training completed for Trial 8 CV 2

