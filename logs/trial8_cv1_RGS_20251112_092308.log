2025-11-12 09:23:08,473 - INFO - Log file: ./logs/trial8_cv1_RGS_20251112_092308.log
2025-11-12 09:23:08,473 - INFO - START TRAINING TRIAL 8 CV 1 - Task: RGS
2025-11-12 09:23:08,473 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 09:23:08,473 - INFO - Loading dataset...
2025-11-12 09:23:08,603 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 09:23:08,603 - INFO - Initializing VIGNet model...
2025-11-12 09:23:08,606 - INFO - Number of batch iterations per epoch: 113
2025-11-12 09:23:16,299 - INFO - Epoch: 1, Training Loss: 0.7626
2025-11-12 09:23:23,543 - INFO - Epoch: 2, Training Loss: 0.5960
2025-11-12 09:23:30,703 - INFO - Epoch: 3, Training Loss: 0.5907
2025-11-12 09:23:38,464 - INFO - Epoch: 4, Training Loss: 0.5918
2025-11-12 09:23:46,586 - INFO - Epoch: 5, Training Loss: 0.5920
2025-11-12 09:23:54,173 - INFO - Epoch: 6, Training Loss: 0.5929
2025-11-12 09:24:02,098 - INFO - Epoch: 7, Training Loss: 0.5912
2025-11-12 09:24:11,273 - INFO - Epoch: 8, Training Loss: 0.5868
2025-11-12 09:24:19,584 - INFO - Epoch: 9, Training Loss: 0.5910
2025-11-12 09:24:27,985 - INFO - Epoch: 10, Training Loss: 0.5889
2025-11-12 09:24:35,842 - INFO - Epoch: 11, Training Loss: 0.5898
2025-11-12 09:24:44,107 - INFO - Epoch: 12, Training Loss: 0.5868
2025-11-12 09:24:52,136 - INFO - Epoch: 13, Training Loss: 0.5880
2025-11-12 09:25:00,345 - INFO - Epoch: 14, Training Loss: 0.5901
2025-11-12 09:25:08,123 - INFO - Epoch: 15, Training Loss: 0.5865
2025-11-12 09:25:16,103 - INFO - Epoch: 16, Training Loss: 0.5860
2025-11-12 09:25:24,410 - INFO - Epoch: 17, Training Loss: 0.5869
2025-11-12 09:25:32,205 - INFO - Epoch: 18, Training Loss: 0.5861
2025-11-12 09:25:39,721 - INFO - Epoch: 19, Training Loss: 0.5846
2025-11-12 09:25:47,342 - INFO - Epoch: 20, Training Loss: 0.5854
2025-11-12 09:25:55,416 - INFO - Epoch: 21, Training Loss: 0.5857
2025-11-12 09:26:03,547 - INFO - Epoch: 22, Training Loss: 0.5871
2025-11-12 09:26:11,320 - INFO - Epoch: 23, Training Loss: 0.5865
2025-11-12 09:26:18,995 - INFO - Epoch: 24, Training Loss: 0.5844
2025-11-12 09:26:27,869 - INFO - Epoch: 25, Training Loss: 0.5806
2025-11-12 09:26:35,282 - INFO - Epoch: 26, Training Loss: 0.5799
2025-11-12 09:26:42,398 - INFO - Epoch: 27, Training Loss: 0.5775
2025-11-12 09:26:50,076 - INFO - Epoch: 28, Training Loss: 0.5677
2025-11-12 09:26:58,610 - INFO - Epoch: 29, Training Loss: 0.5528
2025-11-12 09:27:07,294 - INFO - Epoch: 30, Training Loss: 0.5461
2025-11-12 09:27:14,757 - INFO - Epoch: 31, Training Loss: 0.5096
2025-11-12 09:27:22,256 - INFO - Epoch: 32, Training Loss: 0.4972
2025-11-12 09:27:30,165 - INFO - Epoch: 33, Training Loss: 0.5032
2025-11-12 09:27:37,906 - INFO - Epoch: 34, Training Loss: 0.4948
2025-11-12 09:27:45,748 - INFO - Epoch: 35, Training Loss: 0.4803
2025-11-12 09:27:53,850 - INFO - Epoch: 36, Training Loss: 0.4842
2025-11-12 09:28:01,880 - INFO - Epoch: 37, Training Loss: 0.4851
2025-11-12 09:28:10,133 - INFO - Epoch: 38, Training Loss: 1.2026
2025-11-12 09:28:17,529 - INFO - Epoch: 39, Training Loss: 0.5823
2025-11-12 09:28:25,408 - INFO - Epoch: 40, Training Loss: 0.5795
2025-11-12 09:28:33,312 - INFO - Epoch: 41, Training Loss: 0.5698
2025-11-12 09:28:40,699 - INFO - Epoch: 42, Training Loss: 0.5725
2025-11-12 09:28:48,627 - INFO - Epoch: 43, Training Loss: 0.6087
2025-11-12 09:28:57,312 - INFO - Epoch: 44, Training Loss: 0.5832
2025-11-12 09:29:05,843 - INFO - Epoch: 45, Training Loss: 0.5792
2025-11-12 09:29:14,332 - INFO - Epoch: 46, Training Loss: 0.5800
2025-11-12 09:29:22,443 - INFO - Epoch: 47, Training Loss: 0.5797
2025-11-12 09:29:31,630 - INFO - Epoch: 48, Training Loss: 0.5709
2025-11-12 09:29:39,441 - INFO - Epoch: 49, Training Loss: 0.5534
2025-11-12 09:29:47,747 - INFO - Epoch: 50, Training Loss: 0.5390
2025-11-12 09:29:55,655 - INFO - Epoch: 51, Training Loss: 0.5175
2025-11-12 09:30:03,014 - INFO - Epoch: 52, Training Loss: 0.5338
2025-11-12 09:30:10,816 - INFO - Epoch: 53, Training Loss: 0.5444
2025-11-12 09:30:19,997 - INFO - Epoch: 54, Training Loss: 0.5312
2025-11-12 09:30:27,381 - INFO - Epoch: 55, Training Loss: 0.5112
2025-11-12 09:30:34,678 - INFO - Epoch: 56, Training Loss: 0.5046
2025-11-12 09:30:42,395 - INFO - Epoch: 57, Training Loss: 0.4896
2025-11-12 09:30:50,635 - INFO - Epoch: 58, Training Loss: 0.4902
2025-11-12 09:30:58,271 - INFO - Epoch: 59, Training Loss: 0.4827
2025-11-12 09:31:06,060 - INFO - Epoch: 60, Training Loss: 0.4835
2025-11-12 09:31:15,657 - INFO - Epoch: 61, Training Loss: 0.4833
2025-11-12 09:31:24,410 - INFO - Epoch: 62, Training Loss: 0.4752
2025-11-12 09:31:32,343 - INFO - Epoch: 63, Training Loss: 0.4793
2025-11-12 09:31:39,864 - INFO - Epoch: 64, Training Loss: 0.4804
2025-11-12 09:31:47,456 - INFO - Epoch: 65, Training Loss: 0.4718
2025-11-12 09:31:55,604 - INFO - Epoch: 66, Training Loss: 0.4739
2025-11-12 09:32:03,439 - INFO - Epoch: 67, Training Loss: 0.4702
2025-11-12 09:32:12,527 - INFO - Epoch: 68, Training Loss: 0.4669
2025-11-12 09:32:20,040 - INFO - Epoch: 69, Training Loss: 0.4689
2025-11-12 09:32:28,038 - INFO - Epoch: 70, Training Loss: 0.4695
2025-11-12 09:32:35,837 - INFO - Epoch: 71, Training Loss: 0.4702
2025-11-12 09:32:43,163 - INFO - Epoch: 72, Training Loss: 0.4674
2025-11-12 09:32:50,784 - INFO - Epoch: 73, Training Loss: 0.4705
2025-11-12 09:32:58,875 - INFO - Epoch: 74, Training Loss: 0.4684
2025-11-12 09:33:06,080 - INFO - Epoch: 75, Training Loss: 0.4702
2025-11-12 09:33:15,271 - INFO - Epoch: 76, Training Loss: 0.4653
2025-11-12 09:33:23,435 - INFO - Epoch: 77, Training Loss: 0.4646
2025-11-12 09:33:32,041 - INFO - Epoch: 78, Training Loss: 0.4651
2025-11-12 09:33:40,474 - INFO - Epoch: 79, Training Loss: 0.4763
2025-11-12 09:33:48,596 - INFO - Epoch: 80, Training Loss: 0.4664
2025-11-12 09:33:57,374 - INFO - Epoch: 81, Training Loss: 0.4694
2025-11-12 09:34:05,168 - INFO - Epoch: 82, Training Loss: 0.4630
2025-11-12 09:34:12,939 - INFO - Epoch: 83, Training Loss: 0.4688
2025-11-12 09:34:20,611 - INFO - Epoch: 84, Training Loss: 0.4657
2025-11-12 09:34:29,100 - INFO - Epoch: 85, Training Loss: 0.4678
2025-11-12 09:34:37,891 - INFO - Epoch: 86, Training Loss: 0.4721
2025-11-12 09:34:46,029 - INFO - Epoch: 87, Training Loss: 0.4655
2025-11-12 09:34:54,166 - INFO - Epoch: 88, Training Loss: 0.4635
2025-11-12 09:35:02,122 - INFO - Epoch: 89, Training Loss: 0.4700
2025-11-12 09:35:10,663 - INFO - Epoch: 90, Training Loss: 0.4656
2025-11-12 09:35:18,038 - INFO - Epoch: 91, Training Loss: 0.4644
2025-11-12 09:35:26,178 - INFO - Epoch: 92, Training Loss: 0.4635
2025-11-12 09:35:34,204 - INFO - Epoch: 93, Training Loss: 0.4639
2025-11-12 09:35:42,247 - INFO - Epoch: 94, Training Loss: 0.4658
2025-11-12 09:35:49,949 - INFO - Epoch: 95, Training Loss: 0.4633
2025-11-12 09:35:58,381 - INFO - Epoch: 96, Training Loss: 0.5267
2025-11-12 09:36:06,300 - INFO - Epoch: 97, Training Loss: 0.5098
2025-11-12 09:36:14,708 - INFO - Epoch: 98, Training Loss: 0.4827
2025-11-12 09:36:23,608 - INFO - Epoch: 99, Training Loss: 0.4695
2025-11-12 09:36:32,010 - INFO - Epoch: 100, Training Loss: 0.5102
2025-11-12 09:36:32,010 - INFO - Training completed for Trial 8 CV 1

