2025-11-12 06:30:06,617 - INFO - Log file: ./logs/trial5_cv3_RGS_20251112_063006.log
2025-11-12 06:30:06,617 - INFO - START TRAINING TRIAL 5 CV 3 - Task: RGS
2025-11-12 06:30:06,617 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 06:30:06,617 - INFO - Loading dataset...
2025-11-12 06:30:06,707 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 06:30:06,707 - INFO - Initializing VIGNet model...
2025-11-12 06:30:06,710 - INFO - Number of batch iterations per epoch: 113
2025-11-12 06:30:14,134 - INFO - Epoch: 1, Training Loss: 1.4731
2025-11-12 06:30:21,970 - INFO - Epoch: 2, Training Loss: 0.9456
2025-11-12 06:30:30,055 - INFO - Epoch: 3, Training Loss: 0.7235
2025-11-12 06:30:38,239 - INFO - Epoch: 4, Training Loss: 0.7039
2025-11-12 06:30:45,793 - INFO - Epoch: 5, Training Loss: 0.7078
2025-11-12 06:30:54,435 - INFO - Epoch: 6, Training Loss: 0.7211
2025-11-12 06:31:02,647 - INFO - Epoch: 7, Training Loss: 0.7078
2025-11-12 06:31:10,962 - INFO - Epoch: 8, Training Loss: 0.7036
2025-11-12 06:31:19,801 - INFO - Epoch: 9, Training Loss: 0.7081
2025-11-12 06:31:27,469 - INFO - Epoch: 10, Training Loss: 0.7050
2025-11-12 06:31:34,918 - INFO - Epoch: 11, Training Loss: 0.6972
2025-11-12 06:31:42,966 - INFO - Epoch: 12, Training Loss: 0.6960
2025-11-12 06:31:51,110 - INFO - Epoch: 13, Training Loss: 0.7010
2025-11-12 06:31:59,384 - INFO - Epoch: 14, Training Loss: 0.6968
2025-11-12 06:32:07,016 - INFO - Epoch: 15, Training Loss: 0.6961
2025-11-12 06:32:15,322 - INFO - Epoch: 16, Training Loss: 0.6954
2025-11-12 06:32:22,329 - INFO - Epoch: 17, Training Loss: 0.6950
2025-11-12 06:32:29,768 - INFO - Epoch: 18, Training Loss: 0.7007
2025-11-12 06:32:37,920 - INFO - Epoch: 19, Training Loss: 0.6965
2025-11-12 06:32:45,673 - INFO - Epoch: 20, Training Loss: 0.6932
2025-11-12 06:32:53,759 - INFO - Epoch: 21, Training Loss: 0.6962
2025-11-12 06:33:02,331 - INFO - Epoch: 22, Training Loss: 0.6973
2025-11-12 06:33:10,922 - INFO - Epoch: 23, Training Loss: 0.6958
2025-11-12 06:33:18,788 - INFO - Epoch: 24, Training Loss: 0.6953
2025-11-12 06:33:26,724 - INFO - Epoch: 25, Training Loss: 0.6945
2025-11-12 06:33:34,848 - INFO - Epoch: 26, Training Loss: 0.6935
2025-11-12 06:33:42,783 - INFO - Epoch: 27, Training Loss: 0.6917
2025-11-12 06:33:51,384 - INFO - Epoch: 28, Training Loss: 0.6934
2025-11-12 06:33:59,200 - INFO - Epoch: 29, Training Loss: 0.6907
2025-11-12 06:34:06,740 - INFO - Epoch: 30, Training Loss: 0.6914
2025-11-12 06:34:15,407 - INFO - Epoch: 31, Training Loss: 0.6924
2025-11-12 06:34:23,284 - INFO - Epoch: 32, Training Loss: 0.6914
2025-11-12 06:34:30,656 - INFO - Epoch: 33, Training Loss: 0.6917
2025-11-12 06:34:38,573 - INFO - Epoch: 34, Training Loss: 0.6925
2025-11-12 06:34:46,391 - INFO - Epoch: 35, Training Loss: 0.6892
2025-11-12 06:34:54,904 - INFO - Epoch: 36, Training Loss: 0.6887
2025-11-12 06:35:02,840 - INFO - Epoch: 37, Training Loss: 0.6941
2025-11-12 06:35:10,789 - INFO - Epoch: 38, Training Loss: 0.6919
2025-11-12 06:35:18,506 - INFO - Epoch: 39, Training Loss: 0.6909
2025-11-12 06:35:26,238 - INFO - Epoch: 40, Training Loss: 0.6907
2025-11-12 06:35:34,146 - INFO - Epoch: 41, Training Loss: 0.6953
2025-11-12 06:35:41,517 - INFO - Epoch: 42, Training Loss: 0.6915
2025-11-12 06:35:49,103 - INFO - Epoch: 43, Training Loss: 0.6910
2025-11-12 06:35:57,009 - INFO - Epoch: 44, Training Loss: 0.6912
2025-11-12 06:36:04,396 - INFO - Epoch: 45, Training Loss: 0.6895
2025-11-12 06:36:12,527 - INFO - Epoch: 46, Training Loss: 0.6885
2025-11-12 06:36:20,001 - INFO - Epoch: 47, Training Loss: 0.6878
2025-11-12 06:36:27,121 - INFO - Epoch: 48, Training Loss: 0.6865
2025-11-12 06:36:35,036 - INFO - Epoch: 49, Training Loss: 0.6886
2025-11-12 06:36:42,754 - INFO - Epoch: 50, Training Loss: 0.6851
2025-11-12 06:36:50,602 - INFO - Epoch: 51, Training Loss: 0.6855
2025-11-12 06:36:58,878 - INFO - Epoch: 52, Training Loss: 0.6793
2025-11-12 06:37:06,948 - INFO - Epoch: 53, Training Loss: 0.6625
2025-11-12 06:37:15,289 - INFO - Epoch: 54, Training Loss: 0.6369
2025-11-12 06:37:23,440 - INFO - Epoch: 55, Training Loss: 0.6462
2025-11-12 06:37:31,264 - INFO - Epoch: 56, Training Loss: 0.6429
2025-11-12 06:37:39,887 - INFO - Epoch: 57, Training Loss: 0.7203
2025-11-12 06:37:47,510 - INFO - Epoch: 58, Training Loss: 0.6347
2025-11-12 06:37:55,220 - INFO - Epoch: 59, Training Loss: 0.6201
2025-11-12 06:38:03,197 - INFO - Epoch: 60, Training Loss: 0.5950
2025-11-12 06:38:10,691 - INFO - Epoch: 61, Training Loss: 0.5735
2025-11-12 06:38:17,957 - INFO - Epoch: 62, Training Loss: 0.5748
2025-11-12 06:38:26,282 - INFO - Epoch: 63, Training Loss: 0.5767
2025-11-12 06:38:33,894 - INFO - Epoch: 64, Training Loss: 0.5754
2025-11-12 06:38:41,354 - INFO - Epoch: 65, Training Loss: 0.5683
2025-11-12 06:38:49,969 - INFO - Epoch: 66, Training Loss: 0.5567
2025-11-12 06:38:57,698 - INFO - Epoch: 67, Training Loss: 0.5895
2025-11-12 06:39:06,300 - INFO - Epoch: 68, Training Loss: 0.5615
2025-11-12 06:39:14,640 - INFO - Epoch: 69, Training Loss: 0.5653
2025-11-12 06:39:22,136 - INFO - Epoch: 70, Training Loss: 0.5984
2025-11-12 06:39:30,347 - INFO - Epoch: 71, Training Loss: 0.5538
2025-11-12 06:39:38,289 - INFO - Epoch: 72, Training Loss: 0.6286
2025-11-12 06:39:45,768 - INFO - Epoch: 73, Training Loss: 0.6168
2025-11-12 06:39:54,061 - INFO - Epoch: 74, Training Loss: 0.5971
2025-11-12 06:40:01,404 - INFO - Epoch: 75, Training Loss: 0.5755
2025-11-12 06:40:09,938 - INFO - Epoch: 76, Training Loss: 0.5666
2025-11-12 06:40:18,759 - INFO - Epoch: 77, Training Loss: 0.5674
2025-11-12 06:40:27,430 - INFO - Epoch: 78, Training Loss: 0.5529
2025-11-12 06:40:36,004 - INFO - Epoch: 79, Training Loss: 0.6086
2025-11-12 06:40:43,976 - INFO - Epoch: 80, Training Loss: 0.5534
2025-11-12 06:40:52,142 - INFO - Epoch: 81, Training Loss: 0.5913
2025-11-12 06:41:00,266 - INFO - Epoch: 82, Training Loss: 0.5443
2025-11-12 06:41:07,921 - INFO - Epoch: 83, Training Loss: 0.5592
2025-11-12 06:41:16,521 - INFO - Epoch: 84, Training Loss: 0.5502
2025-11-12 06:41:24,478 - INFO - Epoch: 85, Training Loss: 0.5342
2025-11-12 06:41:31,940 - INFO - Epoch: 86, Training Loss: 0.5441
2025-11-12 06:41:39,083 - INFO - Epoch: 87, Training Loss: 0.5804
2025-11-12 06:41:47,105 - INFO - Epoch: 88, Training Loss: 0.5436
2025-11-12 06:41:54,287 - INFO - Epoch: 89, Training Loss: 0.5426
2025-11-12 06:42:02,206 - INFO - Epoch: 90, Training Loss: 0.5278
2025-11-12 06:42:10,687 - INFO - Epoch: 91, Training Loss: 0.5323
2025-11-12 06:42:18,683 - INFO - Epoch: 92, Training Loss: 0.5319
2025-11-12 06:42:26,397 - INFO - Epoch: 93, Training Loss: 0.5291
2025-11-12 06:42:33,784 - INFO - Epoch: 94, Training Loss: 0.5344
2025-11-12 06:42:41,648 - INFO - Epoch: 95, Training Loss: 0.5450
2025-11-12 06:42:49,729 - INFO - Epoch: 96, Training Loss: 0.5333
2025-11-12 06:42:57,648 - INFO - Epoch: 97, Training Loss: 0.5194
2025-11-12 06:43:06,087 - INFO - Epoch: 98, Training Loss: 0.5228
2025-11-12 06:43:13,394 - INFO - Epoch: 99, Training Loss: 0.5398
2025-11-12 06:43:21,743 - INFO - Epoch: 100, Training Loss: 0.5289
2025-11-12 06:43:21,743 - INFO - Training completed for Trial 5 CV 3

