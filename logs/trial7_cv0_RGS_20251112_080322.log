2025-11-12 08:03:22,066 - INFO - Log file: ./logs/trial7_cv0_RGS_20251112_080322.log
2025-11-12 08:03:22,066 - INFO - START TRAINING TRIAL 7 CV 0 - Task: RGS
2025-11-12 08:03:22,066 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 08:03:22,066 - INFO - Loading dataset...
2025-11-12 08:03:22,179 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 08:03:22,179 - INFO - Initializing VIGNet model...
2025-11-12 08:03:22,182 - INFO - Number of batch iterations per epoch: 113
2025-11-12 08:03:30,024 - INFO - Epoch: 1, Training Loss: 0.9253
2025-11-12 08:03:38,137 - INFO - Epoch: 2, Training Loss: 0.7066
2025-11-12 08:03:46,261 - INFO - Epoch: 3, Training Loss: 0.6957
2025-11-12 08:03:54,238 - INFO - Epoch: 4, Training Loss: 0.6938
2025-11-12 08:04:02,482 - INFO - Epoch: 5, Training Loss: 0.6928
2025-11-12 08:04:10,145 - INFO - Epoch: 6, Training Loss: 0.6903
2025-11-12 08:04:18,147 - INFO - Epoch: 7, Training Loss: 0.6906
2025-11-12 08:04:26,391 - INFO - Epoch: 8, Training Loss: 0.6900
2025-11-12 08:04:34,278 - INFO - Epoch: 9, Training Loss: 0.6900
2025-11-12 08:04:42,519 - INFO - Epoch: 10, Training Loss: 0.6894
2025-11-12 08:04:50,725 - INFO - Epoch: 11, Training Loss: 0.6880
2025-11-12 08:05:00,022 - INFO - Epoch: 12, Training Loss: 0.6890
2025-11-12 08:05:09,437 - INFO - Epoch: 13, Training Loss: 0.6895
2025-11-12 08:05:17,374 - INFO - Epoch: 14, Training Loss: 0.6886
2025-11-12 08:05:24,505 - INFO - Epoch: 15, Training Loss: 0.6882
2025-11-12 08:05:31,779 - INFO - Epoch: 16, Training Loss: 0.6892
2025-11-12 08:05:40,143 - INFO - Epoch: 17, Training Loss: 0.6889
2025-11-12 08:05:47,599 - INFO - Epoch: 18, Training Loss: 0.6903
2025-11-12 08:05:55,140 - INFO - Epoch: 19, Training Loss: 0.6886
2025-11-12 08:06:02,485 - INFO - Epoch: 20, Training Loss: 0.6877
2025-11-12 08:06:10,641 - INFO - Epoch: 21, Training Loss: 0.6890
2025-11-12 08:06:18,366 - INFO - Epoch: 22, Training Loss: 0.6885
2025-11-12 08:06:27,002 - INFO - Epoch: 23, Training Loss: 0.6887
2025-11-12 08:06:35,673 - INFO - Epoch: 24, Training Loss: 0.6894
2025-11-12 08:06:43,389 - INFO - Epoch: 25, Training Loss: 0.6883
2025-11-12 08:06:51,576 - INFO - Epoch: 26, Training Loss: 0.6881
2025-11-12 08:06:59,187 - INFO - Epoch: 27, Training Loss: 0.6878
2025-11-12 08:07:06,483 - INFO - Epoch: 28, Training Loss: 0.6878
2025-11-12 08:07:13,722 - INFO - Epoch: 29, Training Loss: 0.6889
2025-11-12 08:07:21,631 - INFO - Epoch: 30, Training Loss: 0.6886
2025-11-12 08:07:29,662 - INFO - Epoch: 31, Training Loss: 0.6883
2025-11-12 08:07:37,025 - INFO - Epoch: 32, Training Loss: 0.6884
2025-11-12 08:07:44,888 - INFO - Epoch: 33, Training Loss: 0.6885
2025-11-12 08:07:52,880 - INFO - Epoch: 34, Training Loss: 0.6884
2025-11-12 08:08:01,547 - INFO - Epoch: 35, Training Loss: 0.6886
2025-11-12 08:08:09,360 - INFO - Epoch: 36, Training Loss: 0.6887
2025-11-12 08:08:16,958 - INFO - Epoch: 37, Training Loss: 0.6880
2025-11-12 08:08:26,042 - INFO - Epoch: 38, Training Loss: 0.6882
2025-11-12 08:08:33,530 - INFO - Epoch: 39, Training Loss: 0.6877
2025-11-12 08:08:41,062 - INFO - Epoch: 40, Training Loss: 0.6884
2025-11-12 08:08:49,178 - INFO - Epoch: 41, Training Loss: 0.6887
2025-11-12 08:08:56,624 - INFO - Epoch: 42, Training Loss: 0.6880
2025-11-12 08:09:04,429 - INFO - Epoch: 43, Training Loss: 0.6883
2025-11-12 08:09:12,325 - INFO - Epoch: 44, Training Loss: 0.6882
2025-11-12 08:09:20,457 - INFO - Epoch: 45, Training Loss: 0.6879
2025-11-12 08:09:28,652 - INFO - Epoch: 46, Training Loss: 0.6887
2025-11-12 08:09:36,663 - INFO - Epoch: 47, Training Loss: 0.6883
2025-11-12 08:09:45,346 - INFO - Epoch: 48, Training Loss: 0.6880
2025-11-12 08:09:53,179 - INFO - Epoch: 49, Training Loss: 0.6879
2025-11-12 08:10:00,804 - INFO - Epoch: 50, Training Loss: 0.6884
2025-11-12 08:10:08,790 - INFO - Epoch: 51, Training Loss: 0.6882
2025-11-12 08:10:16,601 - INFO - Epoch: 52, Training Loss: 0.6880
2025-11-12 08:10:24,055 - INFO - Epoch: 53, Training Loss: 0.6882
2025-11-12 08:10:32,133 - INFO - Epoch: 54, Training Loss: 0.6880
2025-11-12 08:10:40,150 - INFO - Epoch: 55, Training Loss: 0.6885
2025-11-12 08:10:47,704 - INFO - Epoch: 56, Training Loss: 0.6881
2025-11-12 08:10:56,009 - INFO - Epoch: 57, Training Loss: 0.6883
2025-11-12 08:11:03,926 - INFO - Epoch: 58, Training Loss: 0.6880
2025-11-12 08:11:11,489 - INFO - Epoch: 59, Training Loss: 0.6885
2025-11-12 08:11:19,551 - INFO - Epoch: 60, Training Loss: 0.6882
2025-11-12 08:11:27,660 - INFO - Epoch: 61, Training Loss: 0.6884
2025-11-12 08:11:35,337 - INFO - Epoch: 62, Training Loss: 0.6881
2025-11-12 08:11:42,877 - INFO - Epoch: 63, Training Loss: 0.6887
2025-11-12 08:11:51,148 - INFO - Epoch: 64, Training Loss: 0.6880
2025-11-12 08:11:59,483 - INFO - Epoch: 65, Training Loss: 0.6884
2025-11-12 08:12:08,067 - INFO - Epoch: 66, Training Loss: 0.6877
2025-11-12 08:12:16,094 - INFO - Epoch: 67, Training Loss: 0.6875
2025-11-12 08:12:24,552 - INFO - Epoch: 68, Training Loss: 0.6889
2025-11-12 08:12:33,360 - INFO - Epoch: 69, Training Loss: 0.6884
2025-11-12 08:12:41,591 - INFO - Epoch: 70, Training Loss: 0.6893
2025-11-12 08:12:49,443 - INFO - Epoch: 71, Training Loss: 0.6886
2025-11-12 08:12:57,723 - INFO - Epoch: 72, Training Loss: 0.6878
2025-11-12 08:13:05,877 - INFO - Epoch: 73, Training Loss: 0.6879
2025-11-12 08:13:13,659 - INFO - Epoch: 74, Training Loss: 0.6879
2025-11-12 08:13:22,127 - INFO - Epoch: 75, Training Loss: 0.6871
2025-11-12 08:13:30,758 - INFO - Epoch: 76, Training Loss: 0.6871
2025-11-12 08:13:38,636 - INFO - Epoch: 77, Training Loss: 0.6860
2025-11-12 08:13:46,537 - INFO - Epoch: 78, Training Loss: 0.6854
2025-11-12 08:13:53,871 - INFO - Epoch: 79, Training Loss: 0.6837
2025-11-12 08:14:01,156 - INFO - Epoch: 80, Training Loss: 0.6829
2025-11-12 08:14:09,519 - INFO - Epoch: 81, Training Loss: 0.6795
2025-11-12 08:14:16,831 - INFO - Epoch: 82, Training Loss: 0.6771
2025-11-12 08:14:24,476 - INFO - Epoch: 83, Training Loss: 0.6779
2025-11-12 08:14:32,582 - INFO - Epoch: 84, Training Loss: 0.6770
2025-11-12 08:14:41,541 - INFO - Epoch: 85, Training Loss: 0.6746
2025-11-12 08:14:50,153 - INFO - Epoch: 86, Training Loss: 0.6750
2025-11-12 08:14:57,416 - INFO - Epoch: 87, Training Loss: 0.6766
2025-11-12 08:15:05,721 - INFO - Epoch: 88, Training Loss: 0.6748
2025-11-12 08:15:13,386 - INFO - Epoch: 89, Training Loss: 0.6750
2025-11-12 08:15:21,211 - INFO - Epoch: 90, Training Loss: 0.6760
2025-11-12 08:15:29,398 - INFO - Epoch: 91, Training Loss: 0.6751
2025-11-12 08:15:37,217 - INFO - Epoch: 92, Training Loss: 0.6740
2025-11-12 08:15:45,052 - INFO - Epoch: 93, Training Loss: 0.6728
2025-11-12 08:15:53,646 - INFO - Epoch: 94, Training Loss: 0.6738
2025-11-12 08:16:00,931 - INFO - Epoch: 95, Training Loss: 0.6748
2025-11-12 08:16:09,965 - INFO - Epoch: 96, Training Loss: 0.6725
2025-11-12 08:16:17,665 - INFO - Epoch: 97, Training Loss: 0.6718
2025-11-12 08:16:25,468 - INFO - Epoch: 98, Training Loss: 0.6718
2025-11-12 08:16:33,833 - INFO - Epoch: 99, Training Loss: 0.6721
2025-11-12 08:16:41,499 - INFO - Epoch: 100, Training Loss: 0.6716
2025-11-12 08:16:41,499 - INFO - Training completed for Trial 7 CV 0

