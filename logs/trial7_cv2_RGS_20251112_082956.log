2025-11-12 08:29:56,434 - INFO - Log file: ./logs/trial7_cv2_RGS_20251112_082956.log
2025-11-12 08:29:56,434 - INFO - START TRAINING TRIAL 7 CV 2 - Task: RGS
2025-11-12 08:29:56,434 - INFO - Learning rate: 0.001, Epochs: 100, Batches: 5
2025-11-12 08:29:56,434 - INFO - Loading dataset...
2025-11-12 08:29:56,590 - INFO - Dataset shapes - Train: (567, 17, 25, 1), Valid: (141, 17, 25, 1), Test: (177, 17, 25, 1)
2025-11-12 08:29:56,590 - INFO - Initializing VIGNet model...
2025-11-12 08:29:56,593 - INFO - Number of batch iterations per epoch: 113
2025-11-12 08:30:03,949 - INFO - Epoch: 1, Training Loss: 0.9874
2025-11-12 08:30:11,793 - INFO - Epoch: 2, Training Loss: 0.7020
2025-11-12 08:30:19,776 - INFO - Epoch: 3, Training Loss: 0.6976
2025-11-12 08:30:28,183 - INFO - Epoch: 4, Training Loss: 0.6937
2025-11-12 08:30:36,610 - INFO - Epoch: 5, Training Loss: 0.6922
2025-11-12 08:30:44,334 - INFO - Epoch: 6, Training Loss: 0.6912
2025-11-12 08:30:52,603 - INFO - Epoch: 7, Training Loss: 0.6896
2025-11-12 08:31:00,432 - INFO - Epoch: 8, Training Loss: 0.6901
2025-11-12 08:31:08,386 - INFO - Epoch: 9, Training Loss: 0.6898
2025-11-12 08:31:16,319 - INFO - Epoch: 10, Training Loss: 0.6900
2025-11-12 08:31:24,047 - INFO - Epoch: 11, Training Loss: 0.6903
2025-11-12 08:31:33,066 - INFO - Epoch: 12, Training Loss: 0.6899
2025-11-12 08:31:40,490 - INFO - Epoch: 13, Training Loss: 0.6896
2025-11-12 08:31:48,383 - INFO - Epoch: 14, Training Loss: 0.6895
2025-11-12 08:31:56,326 - INFO - Epoch: 15, Training Loss: 0.6892
2025-11-12 08:32:03,863 - INFO - Epoch: 16, Training Loss: 0.6897
2025-11-12 08:32:11,637 - INFO - Epoch: 17, Training Loss: 0.6895
2025-11-12 08:32:19,697 - INFO - Epoch: 18, Training Loss: 0.6891
2025-11-12 08:32:27,645 - INFO - Epoch: 19, Training Loss: 0.6888
2025-11-12 08:32:35,893 - INFO - Epoch: 20, Training Loss: 0.6891
2025-11-12 08:32:43,278 - INFO - Epoch: 21, Training Loss: 0.6893
2025-11-12 08:32:50,819 - INFO - Epoch: 22, Training Loss: 0.6895
2025-11-12 08:32:59,119 - INFO - Epoch: 23, Training Loss: 0.6898
2025-11-12 08:33:07,591 - INFO - Epoch: 24, Training Loss: 0.6893
2025-11-12 08:33:15,151 - INFO - Epoch: 25, Training Loss: 0.6895
2025-11-12 08:33:23,453 - INFO - Epoch: 26, Training Loss: 0.6885
2025-11-12 08:33:31,408 - INFO - Epoch: 27, Training Loss: 0.6894
2025-11-12 08:33:39,296 - INFO - Epoch: 28, Training Loss: 0.6892
2025-11-12 08:33:47,150 - INFO - Epoch: 29, Training Loss: 0.6892
2025-11-12 08:33:55,741 - INFO - Epoch: 30, Training Loss: 0.6891
2025-11-12 08:34:03,948 - INFO - Epoch: 31, Training Loss: 0.6890
2025-11-12 08:34:12,051 - INFO - Epoch: 32, Training Loss: 0.6892
2025-11-12 08:34:20,684 - INFO - Epoch: 33, Training Loss: 0.6892
2025-11-12 08:34:28,783 - INFO - Epoch: 34, Training Loss: 0.6886
2025-11-12 08:34:37,220 - INFO - Epoch: 35, Training Loss: 0.6893
2025-11-12 08:34:45,269 - INFO - Epoch: 36, Training Loss: 0.6889
2025-11-12 08:34:53,422 - INFO - Epoch: 37, Training Loss: 0.6896
2025-11-12 08:35:01,355 - INFO - Epoch: 38, Training Loss: 0.6894
2025-11-12 08:35:08,675 - INFO - Epoch: 39, Training Loss: 0.6897
2025-11-12 08:35:16,005 - INFO - Epoch: 40, Training Loss: 0.6894
2025-11-12 08:35:23,817 - INFO - Epoch: 41, Training Loss: 0.6893
2025-11-12 08:35:31,219 - INFO - Epoch: 42, Training Loss: 0.6899
2025-11-12 08:35:38,799 - INFO - Epoch: 43, Training Loss: 0.6897
2025-11-12 08:35:48,064 - INFO - Epoch: 44, Training Loss: 0.6896
2025-11-12 08:35:56,002 - INFO - Epoch: 45, Training Loss: 0.6888
2025-11-12 08:36:04,341 - INFO - Epoch: 46, Training Loss: 0.6891
2025-11-12 08:36:12,148 - INFO - Epoch: 47, Training Loss: 0.6891
2025-11-12 08:36:20,117 - INFO - Epoch: 48, Training Loss: 0.6891
2025-11-12 08:36:28,522 - INFO - Epoch: 49, Training Loss: 0.6893
2025-11-12 08:36:36,848 - INFO - Epoch: 50, Training Loss: 0.6894
2025-11-12 08:36:45,282 - INFO - Epoch: 51, Training Loss: 0.6890
2025-11-12 08:36:54,360 - INFO - Epoch: 52, Training Loss: 0.6891
2025-11-12 08:37:01,565 - INFO - Epoch: 53, Training Loss: 0.6890
2025-11-12 08:37:08,880 - INFO - Epoch: 54, Training Loss: 0.6896
2025-11-12 08:37:16,931 - INFO - Epoch: 55, Training Loss: 0.6889
2025-11-12 08:37:25,500 - INFO - Epoch: 56, Training Loss: 0.6891
2025-11-12 08:37:33,153 - INFO - Epoch: 57, Training Loss: 0.6889
2025-11-12 08:37:40,640 - INFO - Epoch: 58, Training Loss: 0.6893
2025-11-12 08:37:48,171 - INFO - Epoch: 59, Training Loss: 0.6888
2025-11-12 08:37:56,691 - INFO - Epoch: 60, Training Loss: 0.6886
2025-11-12 08:38:04,171 - INFO - Epoch: 61, Training Loss: 0.6886
2025-11-12 08:38:12,149 - INFO - Epoch: 62, Training Loss: 0.6884
2025-11-12 08:38:20,762 - INFO - Epoch: 63, Training Loss: 0.6878
2025-11-12 08:38:28,245 - INFO - Epoch: 64, Training Loss: 0.6875
2025-11-12 08:38:35,632 - INFO - Epoch: 65, Training Loss: 0.6848
2025-11-12 08:38:43,375 - INFO - Epoch: 66, Training Loss: 0.6813
2025-11-12 08:38:51,237 - INFO - Epoch: 67, Training Loss: 0.6779
2025-11-12 08:38:58,890 - INFO - Epoch: 68, Training Loss: 0.6767
2025-11-12 08:39:07,080 - INFO - Epoch: 69, Training Loss: 0.6775
2025-11-12 08:39:15,833 - INFO - Epoch: 70, Training Loss: 0.6751
2025-11-12 08:39:24,398 - INFO - Epoch: 71, Training Loss: 0.6754
2025-11-12 08:39:32,686 - INFO - Epoch: 72, Training Loss: 0.6774
2025-11-12 08:39:39,928 - INFO - Epoch: 73, Training Loss: 0.6753
2025-11-12 08:39:47,354 - INFO - Epoch: 74, Training Loss: 0.6979
2025-11-12 08:39:54,805 - INFO - Epoch: 75, Training Loss: 0.6809
2025-11-12 08:40:02,327 - INFO - Epoch: 76, Training Loss: 0.6779
2025-11-12 08:40:10,690 - INFO - Epoch: 77, Training Loss: 0.6740
2025-11-12 08:40:18,097 - INFO - Epoch: 78, Training Loss: 0.6722
2025-11-12 08:40:27,485 - INFO - Epoch: 79, Training Loss: 0.6743
2025-11-12 08:40:35,114 - INFO - Epoch: 80, Training Loss: 0.6758
2025-11-12 08:40:42,839 - INFO - Epoch: 81, Training Loss: 0.6720
2025-11-12 08:40:51,330 - INFO - Epoch: 82, Training Loss: 0.6751
2025-11-12 08:40:59,275 - INFO - Epoch: 83, Training Loss: 0.6947
2025-11-12 08:41:07,107 - INFO - Epoch: 84, Training Loss: 0.6806
2025-11-12 08:41:15,003 - INFO - Epoch: 85, Training Loss: 0.6771
2025-11-12 08:41:23,173 - INFO - Epoch: 86, Training Loss: 0.6744
2025-11-12 08:41:31,790 - INFO - Epoch: 87, Training Loss: 0.6728
2025-11-12 08:41:40,640 - INFO - Epoch: 88, Training Loss: 0.6737
2025-11-12 08:41:48,805 - INFO - Epoch: 89, Training Loss: 0.6717
2025-11-12 08:41:56,541 - INFO - Epoch: 90, Training Loss: 0.6741
2025-11-12 08:42:03,904 - INFO - Epoch: 91, Training Loss: 0.6714
2025-11-12 08:42:11,562 - INFO - Epoch: 92, Training Loss: 0.6733
2025-11-12 08:42:19,737 - INFO - Epoch: 93, Training Loss: 0.6722
2025-11-12 08:42:28,235 - INFO - Epoch: 94, Training Loss: 0.6735
2025-11-12 08:42:36,308 - INFO - Epoch: 95, Training Loss: 0.6724
2025-11-12 08:42:43,958 - INFO - Epoch: 96, Training Loss: 0.6706
2025-11-12 08:42:53,097 - INFO - Epoch: 97, Training Loss: 0.6735
2025-11-12 08:43:01,087 - INFO - Epoch: 98, Training Loss: 0.6708
2025-11-12 08:43:08,401 - INFO - Epoch: 99, Training Loss: 0.6745
2025-11-12 08:43:16,546 - INFO - Epoch: 100, Training Loss: 0.6706
2025-11-12 08:43:16,546 - INFO - Training completed for Trial 7 CV 2

