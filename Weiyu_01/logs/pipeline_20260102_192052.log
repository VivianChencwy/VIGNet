2026-01-02 19:20:52,133 - ======================================================================
2026-01-02 19:20:52,133 - WEIYU DATA PROCESSING AND MODELING PIPELINE
2026-01-02 19:20:52,133 - ======================================================================
2026-01-02 19:20:52,133 - Start time: 2026-01-02 19:20:52
2026-01-02 19:20:52,133 - Log file: logs/pipeline_20260102_192052.log
2026-01-02 19:20:52,133 - 
2026-01-02 19:20:52,133 - ============================================================
2026-01-02 19:20:52,133 - STEP 1: Loading and Trimming Data
2026-01-02 19:20:52,133 - ============================================================
2026-01-02 19:20:52,133 - Loading data from: merged_data.csv
2026-01-02 19:20:53,670 - Loaded 1104038 samples
2026-01-02 19:20:53,670 - Original duration: 2212.86s (36.88 min)
2026-01-02 19:20:53,677 - Original sampling rate: 967.12 Hz
2026-01-02 19:20:53,677 - Trimming: first 300.0s, last 0.0s
2026-01-02 19:20:53,777 - After trimming: 1912.86s (31.88 min)
2026-01-02 19:20:53,777 - Samples after trimming: 954348
2026-01-02 19:20:53,779 - ============================================================
2026-01-02 19:20:53,779 - STEP 2: Resampling
2026-01-02 19:20:53,779 - ============================================================
2026-01-02 19:20:53,779 - Resampling from 967.12 Hz to 200 Hz
2026-01-02 19:20:53,779 - Samples: 954348 -> 197359
2026-01-02 19:20:53,977 - ============================================================
2026-01-02 19:20:53,977 - STEP 3: Segmentation
2026-01-02 19:20:53,977 - ============================================================
2026-01-02 19:20:53,978 - Created 245 windows
2026-01-02 19:20:53,978 - Window shape: 1600 samples x 2 channels
2026-01-02 19:20:53,978 - ============================================================
2026-01-02 19:20:53,978 - STEP 4: Extracting DE Features
2026-01-02 19:20:53,978 - ============================================================
2026-01-02 19:20:53,978 - Extracting DE for 245 windows, 2 channels, 25 bands
2026-01-02 19:20:54,646 -   Processing window 100/245
2026-01-02 19:20:55,317 -   Processing window 200/245
2026-01-02 19:20:55,621 -   Processing window 245/245
2026-01-02 19:20:55,627 - DE features shape: (2, 245, 25)
2026-01-02 19:20:55,627 - ============================================================
2026-01-02 19:20:55,627 - STEP 5: Applying Smoothing
2026-01-02 19:20:55,628 - ============================================================
2026-01-02 19:20:55,628 - Applying moving average (window=5)...
2026-01-02 19:20:55,628 - Applying LDS (Kalman filter) smoothing...
2026-01-02 19:20:56,563 - Smoothing completed
2026-01-02 19:20:56,563 - ============================================================
2026-01-02 19:20:56,563 - STEP 6: Computing PERCLOS Labels
2026-01-02 19:20:56,563 - ============================================================
2026-01-02 19:20:56,754 - Unique eye states in data: ['close' 'open']
2026-01-02 19:20:56,762 - Eye closed samples: 34125.0 / 954348 (3.58%)
2026-01-02 19:20:56,762 - Computing PERCLOS with 60.0s window...
2026-01-02 19:20:56,893 - PERCLOS range: [0.0000, 0.1261]
2026-01-02 19:20:56,893 - PERCLOS mean: 0.0344, std: 0.0339
2026-01-02 19:20:56,893 - Distribution: Awake=245 (100.0%), Tired=0 (0.0%), Drowsy=0 (0.0%)
2026-01-02 19:20:56,893 - ============================================================
2026-01-02 19:20:56,893 - STEP 7: Saving Preprocessed Data
2026-01-02 19:20:56,893 - ============================================================
2026-01-02 19:20:56,894 - Saved DE features: processed/de_features.npy
2026-01-02 19:20:56,894 -   Shape: (245, 2, 25)
2026-01-02 19:20:56,894 - Saved PERCLOS labels: processed/perclos_labels.npy
2026-01-02 19:20:56,894 - Saved timestamps: processed/timestamps.npy
2026-01-02 19:20:56,894 - Saved metadata: processed/metadata.npy
2026-01-02 19:20:56,894 - ============================================================
2026-01-02 19:20:56,894 - STEP 8: Data Splitting (Block-wise Random)
2026-01-02 19:20:56,894 - ============================================================
2026-01-02 19:20:56,894 - Created 24 blocks (block_size=8, gap=2)
2026-01-02 19:20:56,894 - Split results:
2026-01-02 19:20:56,894 -   Train: 128 samples (66.7%)
2026-01-02 19:20:56,894 -   Valid: 24 samples (12.5%)
2026-01-02 19:20:56,894 -   Test:  40 samples (20.8%)
2026-01-02 19:20:56,894 -   Discarded (gaps): 53 samples
2026-01-02 19:20:56,894 - ============================================================
2026-01-02 19:20:56,894 - STEP 9-10: Model Training
2026-01-02 19:20:56,894 - ============================================================
2026-01-02 19:20:56,928 - Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
2026-01-02 19:20:56,928 - Train: (128, 2, 25), Val: (24, 2, 25), Test: (40, 2, 25)
2026-01-02 19:20:56,928 - Applied StandardScaler normalization
2026-01-02 19:20:57,257 - Learning rate: 0.005
2026-01-02 19:20:57,257 - Max epochs: 500
2026-01-02 19:20:57,257 - Batch size: 8
2026-01-02 19:20:57,257 - Early stopping patience: 50
2026-01-02 19:20:58,970 - Epoch   1: Train Loss = 0.002837, Val Loss = 0.000385
2026-01-02 19:20:58,971 -   -> New best validation loss: 0.000385
2026-01-02 19:21:00,098 - Epoch   2: Train Loss = 0.001357, Val Loss = 0.000372
2026-01-02 19:21:00,098 -   -> New best validation loss: 0.000372
2026-01-02 19:21:01,279 - Epoch   3: Train Loss = 0.001076, Val Loss = 0.000363
2026-01-02 19:21:01,280 -   -> New best validation loss: 0.000363
2026-01-02 19:21:02,402 - Epoch   4: Train Loss = 0.000582, Val Loss = 0.000378
2026-01-02 19:21:03,546 - Epoch   5: Train Loss = 0.000776, Val Loss = 0.000365
2026-01-02 19:21:04,662 - Epoch   6: Train Loss = 0.000619, Val Loss = 0.000366
2026-01-02 19:21:05,797 - Epoch   7: Train Loss = 0.000610, Val Loss = 0.000364
2026-01-02 19:21:06,933 - Epoch   8: Train Loss = 0.000459, Val Loss = 0.000460
2026-01-02 19:21:08,045 - Epoch   9: Train Loss = 0.000516, Val Loss = 0.000367
2026-01-02 19:21:09,272 - Epoch  10: Train Loss = 0.000488, Val Loss = 0.000372
2026-01-02 19:21:10,434 - Epoch  11: Train Loss = 0.000478, Val Loss = 0.000424
2026-01-02 19:21:11,609 - Epoch  12: Train Loss = 0.000449, Val Loss = 0.000356
2026-01-02 19:21:11,610 -   -> New best validation loss: 0.000356
2026-01-02 19:21:12,817 - Epoch  13: Train Loss = 0.000391, Val Loss = 0.000644
2026-01-02 19:21:14,067 - Epoch  14: Train Loss = 0.000369, Val Loss = 0.000449
2026-01-02 19:21:15,297 - Epoch  15: Train Loss = 0.000298, Val Loss = 0.000439
2026-01-02 19:21:16,424 - Epoch  16: Train Loss = 0.000345, Val Loss = 0.000421
2026-01-02 19:21:17,585 - Epoch  17: Train Loss = 0.000233, Val Loss = 0.000349
2026-01-02 19:21:17,586 -   -> New best validation loss: 0.000349
2026-01-02 19:21:18,736 - Epoch  18: Train Loss = 0.000283, Val Loss = 0.000365
2026-01-02 19:21:19,874 - Epoch  19: Train Loss = 0.000265, Val Loss = 0.000361
2026-01-02 19:21:21,014 - Epoch  20: Train Loss = 0.000322, Val Loss = 0.000358
2026-01-02 19:21:22,168 - Epoch  21: Train Loss = 0.000235, Val Loss = 0.000523
2026-01-02 19:21:23,300 - Epoch  22: Train Loss = 0.000314, Val Loss = 0.000438
2026-01-02 19:21:24,442 - Epoch  23: Train Loss = 0.000261, Val Loss = 0.000465
2026-01-02 19:21:25,591 - Epoch  24: Train Loss = 0.000214, Val Loss = 0.000350
2026-01-02 19:21:26,894 - Epoch  25: Train Loss = 0.000273, Val Loss = 0.000370
2026-01-02 19:21:28,196 - Epoch  26: Train Loss = 0.000383, Val Loss = 0.000518
2026-01-02 19:21:29,372 - Epoch  27: Train Loss = 0.000225, Val Loss = 0.000782
2026-01-02 19:21:30,575 - Epoch  28: Train Loss = 0.000260, Val Loss = 0.000348
2026-01-02 19:21:30,576 -   -> New best validation loss: 0.000348
2026-01-02 19:21:31,753 - Epoch  29: Train Loss = 0.000195, Val Loss = 0.000408
2026-01-02 19:21:33,004 - Epoch  30: Train Loss = 0.000273, Val Loss = 0.000357
2026-01-02 19:21:34,145 - Epoch  31: Train Loss = 0.000232, Val Loss = 0.000469
2026-01-02 19:21:35,362 - Epoch  32: Train Loss = 0.000218, Val Loss = 0.000585
2026-01-02 19:21:36,550 - Epoch  33: Train Loss = 0.000225, Val Loss = 0.000516
2026-01-02 19:21:37,701 - Epoch  34: Train Loss = 0.000235, Val Loss = 0.000340
2026-01-02 19:21:37,702 -   -> New best validation loss: 0.000340
2026-01-02 19:21:38,819 - Epoch  35: Train Loss = 0.000231, Val Loss = 0.000489
2026-01-02 19:21:39,949 - Epoch  36: Train Loss = 0.000217, Val Loss = 0.000575
2026-01-02 19:21:41,090 - Epoch  37: Train Loss = 0.000194, Val Loss = 0.000384
2026-01-02 19:21:42,225 - Epoch  38: Train Loss = 0.000239, Val Loss = 0.000645
2026-01-02 19:21:43,365 - Epoch  39: Train Loss = 0.000234, Val Loss = 0.000457
2026-01-02 19:21:44,561 - Epoch  40: Train Loss = 0.000227, Val Loss = 0.000633
2026-01-02 19:21:45,741 - Epoch  41: Train Loss = 0.000218, Val Loss = 0.000443
2026-01-02 19:21:46,949 - Epoch  42: Train Loss = 0.000207, Val Loss = 0.000657
2026-01-02 19:21:48,078 - Epoch  43: Train Loss = 0.000191, Val Loss = 0.000478
2026-01-02 19:21:49,200 - Epoch  44: Train Loss = 0.000212, Val Loss = 0.000535
2026-01-02 19:21:50,360 - Epoch  45: Train Loss = 0.000189, Val Loss = 0.000521
2026-01-02 19:21:51,623 - Epoch  46: Train Loss = 0.000137, Val Loss = 0.000742
2026-01-02 19:21:52,747 - Epoch  47: Train Loss = 0.000167, Val Loss = 0.000630
2026-01-02 19:21:53,919 - Epoch  48: Train Loss = 0.000169, Val Loss = 0.000698
2026-01-02 19:21:55,162 - Epoch  49: Train Loss = 0.000168, Val Loss = 0.000700
2026-01-02 19:21:56,297 - Epoch  50: Train Loss = 0.000221, Val Loss = 0.000549
2026-01-02 19:21:57,442 - Epoch  51: Train Loss = 0.000238, Val Loss = 0.001185
2026-01-02 19:21:58,575 - Epoch  52: Train Loss = 0.000347, Val Loss = 0.000900
2026-01-02 19:21:59,699 - Epoch  53: Train Loss = 0.000309, Val Loss = 0.000402
2026-01-02 19:22:00,888 - Epoch  54: Train Loss = 0.000157, Val Loss = 0.000858
2026-01-02 19:22:02,140 - Epoch  55: Train Loss = 0.000182, Val Loss = 0.000911
2026-01-02 19:22:03,298 - Epoch  56: Train Loss = 0.000229, Val Loss = 0.000606
2026-01-02 19:22:04,456 - Epoch  57: Train Loss = 0.000130, Val Loss = 0.000627
2026-01-02 19:22:05,586 - Epoch  58: Train Loss = 0.000157, Val Loss = 0.000659
2026-01-02 19:22:06,855 - Epoch  59: Train Loss = 0.000183, Val Loss = 0.000686
2026-01-02 19:22:08,028 - Epoch  60: Train Loss = 0.000113, Val Loss = 0.000932
2026-01-02 19:22:09,235 - Epoch  61: Train Loss = 0.000230, Val Loss = 0.000576
2026-01-02 19:22:10,444 - Epoch  62: Train Loss = 0.000184, Val Loss = 0.000414
2026-01-02 19:22:11,595 - Epoch  63: Train Loss = 0.000203, Val Loss = 0.000550
2026-01-02 19:22:12,815 - Epoch  64: Train Loss = 0.000178, Val Loss = 0.000531
2026-01-02 19:22:13,984 - Epoch  65: Train Loss = 0.000166, Val Loss = 0.000757
2026-01-02 19:22:15,136 - Epoch  66: Train Loss = 0.000158, Val Loss = 0.000599
2026-01-02 19:22:16,344 - Epoch  67: Train Loss = 0.000153, Val Loss = 0.000646
2026-01-02 19:22:17,615 - Epoch  68: Train Loss = 0.000152, Val Loss = 0.000689
2026-01-02 19:22:18,816 - Epoch  69: Train Loss = 0.000160, Val Loss = 0.000519
2026-01-02 19:22:19,979 - Epoch  70: Train Loss = 0.000123, Val Loss = 0.000515
2026-01-02 19:22:21,135 - Epoch  71: Train Loss = 0.000210, Val Loss = 0.000896
2026-01-02 19:22:22,321 - Epoch  72: Train Loss = 0.000196, Val Loss = 0.000662
2026-01-02 19:22:23,686 - Epoch  73: Train Loss = 0.000184, Val Loss = 0.000997
2026-01-02 19:22:24,916 - Epoch  74: Train Loss = 0.000168, Val Loss = 0.000768
2026-01-02 19:22:26,054 - Epoch  75: Train Loss = 0.000140, Val Loss = 0.000523
2026-01-02 19:22:27,310 - Epoch  76: Train Loss = 0.000188, Val Loss = 0.000751
2026-01-02 19:22:28,502 - Epoch  77: Train Loss = 0.000121, Val Loss = 0.000659
2026-01-02 19:22:29,785 - Epoch  78: Train Loss = 0.000135, Val Loss = 0.000762
2026-01-02 19:22:30,926 - Epoch  79: Train Loss = 0.000126, Val Loss = 0.000635
2026-01-02 19:22:32,051 - Epoch  80: Train Loss = 0.000151, Val Loss = 0.000482
2026-01-02 19:22:33,192 - Epoch  81: Train Loss = 0.000145, Val Loss = 0.000648
2026-01-02 19:22:34,351 - Epoch  82: Train Loss = 0.000158, Val Loss = 0.000779
2026-01-02 19:22:35,490 - Epoch  83: Train Loss = 0.000154, Val Loss = 0.000510
2026-01-02 19:22:36,649 - Epoch  84: Train Loss = 0.000140, Val Loss = 0.000662
2026-01-02 19:22:36,649 - Early stopping at epoch 84
2026-01-02 19:22:36,653 - Restored best model weights
2026-01-02 19:22:36,653 - 
============================================================
2026-01-02 19:22:36,653 - EVALUATION RESULTS
2026-01-02 19:22:36,653 - ============================================================
2026-01-02 19:22:36,697 - Validation: MSE=0.000335, MAE=0.015528, RMSE=0.018303, Corr=0.460329
2026-01-02 19:22:36,743 - Test:       MSE=0.000164, MAE=0.010616, RMSE=0.012810, Corr=0.909833
2026-01-02 19:22:36,743 - 
Saving model and results...
2026-01-02 19:22:36,997 - Saved weights to: logs/models/best_weights.h5
2026-01-02 19:22:36,997 - Saved scaler to: logs/models/scaler.pkl
2026-01-02 19:22:36,997 - Saved predictions to: logs/models/predictions.npy
2026-01-02 19:22:36,997 - ============================================================
2026-01-02 19:22:36,997 - STEP 11: Plotting Prediction Results
2026-01-02 19:22:36,997 - ============================================================
2026-01-02 19:22:37,923 - Saved prediction plots to: logs/perclos_prediction_results.png
2026-01-02 19:22:37,923 - 
2026-01-02 19:22:37,923 - ======================================================================
2026-01-02 19:22:37,923 - PIPELINE COMPLETED SUCCESSFULLY
2026-01-02 19:22:37,923 - ======================================================================
2026-01-02 19:22:37,923 - End time: 2026-01-02 19:22:37
2026-01-02 19:22:37,923 - Duration: 0:01:45.789582
2026-01-02 19:22:37,923 - 
2026-01-02 19:22:37,923 - Output files:
2026-01-02 19:22:37,923 -   - Preprocessed data: processed/
2026-01-02 19:22:37,923 -   - Model and results: logs/models/
2026-01-02 19:22:37,923 -   - Prediction plots: logs/perclos_prediction_results.png
2026-01-02 19:22:37,923 -   - Log file: logs/pipeline_20260102_192052.log
