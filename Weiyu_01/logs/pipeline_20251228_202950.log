2025-12-28 20:29:50,006 - ======================================================================
2025-12-28 20:29:50,006 - WEIYU DATA PROCESSING AND MODELING PIPELINE
2025-12-28 20:29:50,006 - ======================================================================
2025-12-28 20:29:50,006 - Start time: 2025-12-28 20:29:50
2025-12-28 20:29:50,006 - Log file: logs/pipeline_20251228_202950.log
2025-12-28 20:29:50,006 - 
2025-12-28 20:29:50,006 - ============================================================
2025-12-28 20:29:50,006 - STEP 1: Loading and Trimming Data
2025-12-28 20:29:50,006 - ============================================================
2025-12-28 20:29:50,006 - Loading data from: merged_data.csv
2025-12-28 20:29:51,427 - Loaded 1104038 samples
2025-12-28 20:29:51,427 - Original duration: 2212.86s (36.88 min)
2025-12-28 20:29:51,433 - Original sampling rate: 967.12 Hz
2025-12-28 20:29:51,433 - Trimming: first 300.0s, last 0.0s
2025-12-28 20:29:51,533 - After trimming: 1912.86s (31.88 min)
2025-12-28 20:29:51,533 - Samples after trimming: 954348
2025-12-28 20:29:51,536 - ============================================================
2025-12-28 20:29:51,536 - STEP 2: Resampling
2025-12-28 20:29:51,536 - ============================================================
2025-12-28 20:29:51,536 - Resampling from 967.12 Hz to 200 Hz
2025-12-28 20:29:51,536 - Samples: 954348 -> 197359
2025-12-28 20:29:51,733 - ============================================================
2025-12-28 20:29:51,733 - STEP 3: Segmentation
2025-12-28 20:29:51,733 - ============================================================
2025-12-28 20:29:51,734 - Created 245 windows
2025-12-28 20:29:51,734 - Window shape: 1600 samples x 2 channels
2025-12-28 20:29:51,734 - ============================================================
2025-12-28 20:29:51,734 - STEP 4: Extracting DE Features
2025-12-28 20:29:51,734 - ============================================================
2025-12-28 20:29:51,734 - Extracting DE for 245 windows, 2 channels, 25 bands
2025-12-28 20:29:52,407 -   Processing window 100/245
2025-12-28 20:29:53,083 -   Processing window 200/245
2025-12-28 20:29:53,386 -   Processing window 245/245
2025-12-28 20:29:53,393 - DE features shape: (2, 245, 25)
2025-12-28 20:29:53,393 - ============================================================
2025-12-28 20:29:53,393 - STEP 5: Applying Smoothing
2025-12-28 20:29:53,393 - ============================================================
2025-12-28 20:29:53,393 - Applying moving average (window=5)...
2025-12-28 20:29:53,393 - Applying LDS (Kalman filter) smoothing...
2025-12-28 20:29:54,311 - Smoothing completed
2025-12-28 20:29:54,312 - ============================================================
2025-12-28 20:29:54,312 - STEP 6: Computing PERCLOS Labels
2025-12-28 20:29:54,312 - ============================================================
2025-12-28 20:29:54,316 - Computing PERCLOS with 60.0s window...
2025-12-28 20:29:54,444 - PERCLOS range: [0.0000, 0.0000]
2025-12-28 20:29:54,444 - PERCLOS mean: 0.0000, std: 0.0000
2025-12-28 20:29:54,444 - Distribution: Awake=245 (100.0%), Tired=0 (0.0%), Drowsy=0 (0.0%)
2025-12-28 20:29:54,444 - ============================================================
2025-12-28 20:29:54,444 - STEP 7: Saving Preprocessed Data
2025-12-28 20:29:54,444 - ============================================================
2025-12-28 20:29:54,445 - Saved DE features: processed/de_features.npy
2025-12-28 20:29:54,445 -   Shape: (245, 2, 25)
2025-12-28 20:29:54,445 - Saved PERCLOS labels: processed/perclos_labels.npy
2025-12-28 20:29:54,445 - Saved timestamps: processed/timestamps.npy
2025-12-28 20:29:54,445 - Saved metadata: processed/metadata.npy
2025-12-28 20:29:54,445 - ============================================================
2025-12-28 20:29:54,445 - STEP 8: Data Splitting (Block-wise Random)
2025-12-28 20:29:54,445 - ============================================================
2025-12-28 20:29:54,445 - Created 24 blocks (block_size=8, gap=2)
2025-12-28 20:29:54,445 - Split results:
2025-12-28 20:29:54,445 -   Train: 128 samples (66.7%)
2025-12-28 20:29:54,445 -   Valid: 24 samples (12.5%)
2025-12-28 20:29:54,445 -   Test:  40 samples (20.8%)
2025-12-28 20:29:54,445 -   Discarded (gaps): 53 samples
2025-12-28 20:29:54,445 - ============================================================
2025-12-28 20:29:54,445 - STEP 9-10: Model Training
2025-12-28 20:29:54,445 - ============================================================
2025-12-28 20:29:54,511 - Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
2025-12-28 20:29:54,511 - Train: (128, 2, 25), Val: (24, 2, 25), Test: (40, 2, 25)
2025-12-28 20:29:54,512 - Applied StandardScaler normalization
2025-12-28 20:29:55,048 - Learning rate: 0.005
2025-12-28 20:29:55,048 - Max epochs: 500
2025-12-28 20:29:55,048 - Batch size: 8
2025-12-28 20:29:55,048 - Early stopping patience: 50
2025-12-28 20:29:57,073 - Epoch   1: Train Loss = 0.002391, Val Loss = 0.000001
2025-12-28 20:29:57,074 -   -> New best validation loss: 0.000001
2025-12-28 20:29:58,258 - Epoch   2: Train Loss = 0.000862, Val Loss = 0.000035
2025-12-28 20:29:59,463 - Epoch   3: Train Loss = 0.000298, Val Loss = 0.000005
2025-12-28 20:30:00,587 - Epoch   4: Train Loss = 0.000224, Val Loss = 0.000026
2025-12-28 20:30:01,712 - Epoch   5: Train Loss = 0.000128, Val Loss = 0.000001
2025-12-28 20:30:02,881 - Epoch   6: Train Loss = 0.000104, Val Loss = 0.000001
2025-12-28 20:30:04,030 - Epoch   7: Train Loss = 0.000065, Val Loss = 0.000000
2025-12-28 20:30:04,031 -   -> New best validation loss: 0.000000
2025-12-28 20:30:05,170 - Epoch   8: Train Loss = 0.000062, Val Loss = 0.000004
2025-12-28 20:30:06,293 - Epoch   9: Train Loss = 0.000070, Val Loss = 0.000000
2025-12-28 20:30:07,426 - Epoch  10: Train Loss = 0.000043, Val Loss = 0.000006
2025-12-28 20:30:08,572 - Epoch  11: Train Loss = 0.000034, Val Loss = 0.000001
2025-12-28 20:30:09,703 - Epoch  12: Train Loss = 0.000024, Val Loss = 0.000005
2025-12-28 20:30:10,834 - Epoch  13: Train Loss = 0.000030, Val Loss = 0.000004
2025-12-28 20:30:11,976 - Epoch  14: Train Loss = 0.000027, Val Loss = 0.000002
2025-12-28 20:30:13,222 - Epoch  15: Train Loss = 0.000020, Val Loss = 0.000000
2025-12-28 20:30:14,383 - Epoch  16: Train Loss = 0.000017, Val Loss = 0.000006
2025-12-28 20:30:15,573 - Epoch  17: Train Loss = 0.000021, Val Loss = 0.000000
2025-12-28 20:30:15,574 -   -> New best validation loss: 0.000000
2025-12-28 20:30:16,804 - Epoch  18: Train Loss = 0.000020, Val Loss = 0.000006
2025-12-28 20:30:17,996 - Epoch  19: Train Loss = 0.000019, Val Loss = 0.000004
2025-12-28 20:30:19,140 - Epoch  20: Train Loss = 0.000014, Val Loss = 0.000000
2025-12-28 20:30:20,290 - Epoch  21: Train Loss = 0.000013, Val Loss = 0.000004
2025-12-28 20:30:21,440 - Epoch  22: Train Loss = 0.000015, Val Loss = 0.000007
2025-12-28 20:30:22,584 - Epoch  23: Train Loss = 0.000017, Val Loss = 0.000002
2025-12-28 20:30:23,739 - Epoch  24: Train Loss = 0.000011, Val Loss = 0.000009
2025-12-28 20:30:24,888 - Epoch  25: Train Loss = 0.000011, Val Loss = 0.000000
2025-12-28 20:30:26,084 - Epoch  26: Train Loss = 0.000006, Val Loss = 0.000000
2025-12-28 20:30:27,297 - Epoch  27: Train Loss = 0.000007, Val Loss = 0.000003
2025-12-28 20:30:28,491 - Epoch  28: Train Loss = 0.000011, Val Loss = 0.000007
2025-12-28 20:30:29,674 - Epoch  29: Train Loss = 0.000008, Val Loss = 0.000004
2025-12-28 20:30:30,850 - Epoch  30: Train Loss = 0.000007, Val Loss = 0.000001
2025-12-28 20:30:32,040 - Epoch  31: Train Loss = 0.000006, Val Loss = 0.000002
2025-12-28 20:30:33,174 - Epoch  32: Train Loss = 0.000008, Val Loss = 0.000000
2025-12-28 20:30:34,400 - Epoch  33: Train Loss = 0.000006, Val Loss = 0.000001
2025-12-28 20:30:35,641 - Epoch  34: Train Loss = 0.000004, Val Loss = 0.000000
2025-12-28 20:30:36,920 - Epoch  35: Train Loss = 0.000005, Val Loss = 0.000001
2025-12-28 20:30:38,195 - Epoch  36: Train Loss = 0.000005, Val Loss = 0.000000
2025-12-28 20:30:39,338 - Epoch  37: Train Loss = 0.000004, Val Loss = 0.000000
2025-12-28 20:30:40,514 - Epoch  38: Train Loss = 0.000005, Val Loss = 0.000000
2025-12-28 20:30:41,656 - Epoch  39: Train Loss = 0.000004, Val Loss = 0.000001
2025-12-28 20:30:42,793 - Epoch  40: Train Loss = 0.000005, Val Loss = 0.000003
2025-12-28 20:30:43,920 - Epoch  41: Train Loss = 0.000005, Val Loss = 0.000001
2025-12-28 20:30:45,054 - Epoch  42: Train Loss = 0.000006, Val Loss = 0.000002
2025-12-28 20:30:46,188 - Epoch  43: Train Loss = 0.000004, Val Loss = 0.000004
2025-12-28 20:30:47,377 - Epoch  44: Train Loss = 0.000004, Val Loss = 0.000000
2025-12-28 20:30:48,601 - Epoch  45: Train Loss = 0.000003, Val Loss = 0.000002
2025-12-28 20:30:49,853 - Epoch  46: Train Loss = 0.000005, Val Loss = 0.000001
2025-12-28 20:30:51,013 - Epoch  47: Train Loss = 0.000003, Val Loss = 0.000000
2025-12-28 20:30:52,231 - Epoch  48: Train Loss = 0.000003, Val Loss = 0.000000
2025-12-28 20:30:53,483 - Epoch  49: Train Loss = 0.000002, Val Loss = 0.000002
2025-12-28 20:30:54,670 - Epoch  50: Train Loss = 0.000003, Val Loss = 0.000001
2025-12-28 20:30:55,843 - Epoch  51: Train Loss = 0.000002, Val Loss = 0.000002
2025-12-28 20:30:57,068 - Epoch  52: Train Loss = 0.000003, Val Loss = 0.000000
2025-12-28 20:30:58,251 - Epoch  53: Train Loss = 0.000002, Val Loss = 0.000001
2025-12-28 20:30:59,407 - Epoch  54: Train Loss = 0.000003, Val Loss = 0.000000
2025-12-28 20:31:00,613 - Epoch  55: Train Loss = 0.000001, Val Loss = 0.000000
2025-12-28 20:31:01,828 - Epoch  56: Train Loss = 0.000002, Val Loss = 0.000000
2025-12-28 20:31:03,086 - Epoch  57: Train Loss = 0.000002, Val Loss = 0.000001
2025-12-28 20:31:04,228 - Epoch  58: Train Loss = 0.000004, Val Loss = 0.000000
2025-12-28 20:31:05,385 - Epoch  59: Train Loss = 0.000002, Val Loss = 0.000000
2025-12-28 20:31:06,551 - Epoch  60: Train Loss = 0.000002, Val Loss = 0.000000
2025-12-28 20:31:07,717 - Epoch  61: Train Loss = 0.000002, Val Loss = 0.000000
2025-12-28 20:31:08,964 - Epoch  62: Train Loss = 0.000002, Val Loss = 0.000001
2025-12-28 20:31:10,199 - Epoch  63: Train Loss = 0.000002, Val Loss = 0.000000
2025-12-28 20:31:11,396 - Epoch  64: Train Loss = 0.000001, Val Loss = 0.000000
2025-12-28 20:31:12,548 - Epoch  65: Train Loss = 0.000002, Val Loss = 0.000000
2025-12-28 20:31:13,703 - Epoch  66: Train Loss = 0.000002, Val Loss = 0.000000
2025-12-28 20:31:14,909 - Epoch  67: Train Loss = 0.000002, Val Loss = 0.000000
2025-12-28 20:31:14,909 - Early stopping at epoch 67
2025-12-28 20:31:14,913 - Restored best model weights
2025-12-28 20:31:14,913 - 
============================================================
2025-12-28 20:31:14,913 - EVALUATION RESULTS
2025-12-28 20:31:14,913 - ============================================================
2025-12-28 20:31:14,959 - Validation: MSE=0.000000, MAE=0.000040, RMSE=0.000047, Corr=nan
2025-12-28 20:31:15,009 - Test:       MSE=0.000000, MAE=0.000101, RMSE=0.000120, Corr=nan
2025-12-28 20:31:15,009 - 
Saving model and results...
2025-12-28 20:31:15,281 - Saved weights to: logs/models/best_weights.h5
2025-12-28 20:31:15,281 - Saved scaler to: logs/models/scaler.pkl
2025-12-28 20:31:15,281 - Saved predictions to: logs/models/predictions.npy
2025-12-28 20:31:15,281 - 
2025-12-28 20:31:15,281 - ======================================================================
2025-12-28 20:31:15,281 - PIPELINE COMPLETED SUCCESSFULLY
2025-12-28 20:31:15,281 - ======================================================================
2025-12-28 20:31:15,281 - End time: 2025-12-28 20:31:15
2025-12-28 20:31:15,281 - Duration: 0:01:25.275060
2025-12-28 20:31:15,281 - 
2025-12-28 20:31:15,281 - Output files:
2025-12-28 20:31:15,281 -   - Preprocessed data: processed/
2025-12-28 20:31:15,281 -   - Model and results: logs/models/
2025-12-28 20:31:15,281 -   - Log file: logs/pipeline_20251228_202950.log
